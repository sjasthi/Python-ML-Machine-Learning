{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# üéØ Cross-Validation in Machine Learning\n",
        "### Siva.Jasthi@metrostate.edu\n",
        "### Machine Learning and Data Mining\n",
        "\n",
        "---\n",
        "\n",
        "## üìö What You'll Learn Today\n",
        "\n",
        "In this notebook, you'll discover:\n",
        "- What cross-validation is and why it's crucial for ML\n",
        "- Different types of cross-validation techniques\n",
        "- When to use each technique\n",
        "- How to implement them in Python\n",
        "- How to interpret the results"
      ],
      "metadata": {
        "id": "uoYlgH-EDwoJ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "## ü§î What is Cross-Validation?\n",
        "\n",
        "### The Problem\n",
        "Imagine you're studying for a test. If you only practice with the exact same questions that will be on the test, you might memorize the answers without really learning. That's called **overfitting** in machine learning!\n",
        "\n",
        "### The Solution: Cross-Validation\n",
        "Cross-validation is like practicing with different practice tests to make sure you really understand the material, not just memorize specific questions.\n",
        "\n",
        "### Real-World Analogy üéÆ\n",
        "Think of it like testing a video game:\n",
        "- **Bad approach:** Only test one level over and over\n",
        "- **Good approach (Cross-Validation):** Test different levels to make sure the game works everywhere\n",
        "\n",
        "### Why Do We Need It?\n",
        "1. **Reliability:** Get a better estimate of how your model performs on new data\n",
        "2. **Fairness:** Test on multiple different subsets of data\n",
        "3. **Confidence:** Know how consistent your model's performance is\n",
        "4. **Optimization:** Compare different models fairly\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "intro_cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìä Types of Cross-Validation\n",
        "\n",
        "We'll explore these techniques:\n",
        "\n",
        "| Technique | When to Use | Pros | Cons |\n",
        "|-----------|-------------|------|------|\n",
        "| **K-Fold** | Most situations | Fast, reliable | May miss patterns |\n",
        "| **Stratified K-Fold** | Imbalanced classes | Preserves class distribution | Only for classification |\n",
        "| **Leave-One-Out (LOO)** | Small datasets | Uses all data | Very slow |\n",
        "| **Leave-P-Out (LPO)** | Very small datasets | Thorough | Extremely slow |\n",
        "| **Time Series Split** | Time-based data | Respects time order | Only for time series |\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "types_cv"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üîß Setup: Import Libraries\n",
        "\n",
        "Let's import all the tools we'll need for our cross-validation journey!"
      ],
      "metadata": {
        "id": "setup"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Import cross-validation tools\n",
        "from sklearn.model_selection import (\n",
        "    cross_val_score,      # Function to perform cross-validation\n",
        "    KFold,                # K-Fold CV\n",
        "    StratifiedKFold,      # Stratified K-Fold CV\n",
        "    LeaveOneOut,          # Leave-One-Out CV\n",
        "    LeavePOut,            # Leave-P-Out CV\n",
        "    TimeSeriesSplit       # Time Series CV\n",
        ")\n",
        "\n",
        "# Import machine learning models\n",
        "from sklearn.linear_model import LogisticRegression, LinearRegression\n",
        "from sklearn.tree import DecisionTreeClassifier, DecisionTreeRegressor\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "\n",
        "# Import datasets\n",
        "from sklearn.datasets import (\n",
        "    make_classification,      # Create synthetic classification data\n",
        "    load_iris,                # Classic flower classification dataset\n",
        "    fetch_california_housing, # Housing price prediction dataset\n",
        "    load_wine                 # Wine classification dataset\n",
        ")\n",
        "\n",
        "# Import utilities\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "# Set random seed for reproducibility (so we all get the same results!)\n",
        "np.random.seed(42)\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")\n",
        "print(\"üöÄ Ready to explore Cross-Validation!\")"
      ],
      "metadata": {
        "id": "imports"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 1Ô∏è‚É£ K-Fold Cross-Validation\n",
        "\n",
        "## üéØ The Main Technique\n",
        "\n",
        "### How It Works (Deck of Cards Analogy üÉè)\n",
        "Imagine you have a deck of 100 cards:\n",
        "1. **Shuffle** the deck\n",
        "2. **Divide** into 5 equal piles (20 cards each)\n",
        "3. **Test** using one pile, **train** using the other 4 piles\n",
        "4. **Repeat** 5 times, using each pile as the test set once\n",
        "5. **Average** all 5 scores for final result\n",
        "\n",
        "### Visual Representation\n",
        "```\n",
        "Fold 1: [TEST] [TRAIN] [TRAIN] [TRAIN] [TRAIN]\n",
        "Fold 2: [TRAIN] [TEST] [TRAIN] [TRAIN] [TRAIN]\n",
        "Fold 3: [TRAIN] [TRAIN] [TEST] [TRAIN] [TRAIN]\n",
        "Fold 4: [TRAIN] [TRAIN] [TRAIN] [TEST] [TRAIN]\n",
        "Fold 5: [TRAIN] [TRAIN] [TRAIN] [TRAIN] [TEST]\n",
        "```\n",
        "\n",
        "### Key Parameters\n",
        "- `n_splits=5`: Number of folds (commonly 5 or 10)\n",
        "- `shuffle=True`: Randomly mix data before splitting\n",
        "- `random_state=42`: Ensures same shuffle every time (reproducibility)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "kfold_intro"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## üìù Example 1: K-Fold on Logistic Regression\n",
        "\n",
        "Let's classify data into two categories (like spam vs. not spam emails)"
      ],
      "metadata": {
        "id": "ZGz--z8KEBpy"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sOQfc7ZrDsEu"
      },
      "outputs": [],
      "source": [
        "print(\"=\"*60)\n",
        "print(\"K-FOLD CROSS-VALIDATION ON LOGISTIC REGRESSION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# STEP 1: Create a synthetic dataset for binary classification\n",
        "# Think of this as creating fake data about two groups (like cats vs dogs)\n",
        "X, y = make_classification(\n",
        "    n_samples=100,        # 100 data points\n",
        "    n_features=10,        # 10 measurements per data point\n",
        "    n_classes=2,          # 2 categories (binary classification)\n",
        "    n_informative=8,      # 8 features are actually useful\n",
        "    n_redundant=2,        # 2 features are just noise\n",
        "    random_state=42       # For reproducibility\n",
        ")\n",
        "\n",
        "print(f\"üìä Dataset created: {X.shape[0]} samples, {X.shape[1]} features\")\n",
        "print(f\"üìà Classes distribution: {np.bincount(y)}\")\n",
        "print()\n",
        "\n",
        "# STEP 2: Create the model\n",
        "# Logistic Regression is good for yes/no, true/false predictions\n",
        "model = LogisticRegression(max_iter=1000)  # max_iter prevents warnings\n",
        "\n",
        "# STEP 3: Set up K-Fold Cross-Validation\n",
        "kfold = KFold(\n",
        "    n_splits=5,           # Split data into 5 parts\n",
        "    shuffle=True,         # Randomly shuffle before splitting\n",
        "    random_state=42       # Same shuffle every time\n",
        ")\n",
        "\n",
        "# STEP 4: Run cross-validation\n",
        "# This trains and tests the model 5 times, each time with different test data\n",
        "results = cross_val_score(\n",
        "    model,                # The model to test\n",
        "    X,                    # The features\n",
        "    y,                    # The labels\n",
        "    cv=kfold,             # The cross-validation strategy\n",
        "    scoring='accuracy'    # How to measure success (% correct)\n",
        ")\n",
        "\n",
        "# STEP 5: Display and interpret results\n",
        "print(\"üîç K-Fold Cross-Validation Results:\")\n",
        "print(\"-\" * 50)\n",
        "for i, score in enumerate(results, 1):\n",
        "    print(f\"Fold {i}: {score:.3f} ({score*100:.1f}% accuracy)\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"\\nüìä Summary Statistics:\")\n",
        "print(f\"   Mean Accuracy: {results.mean():.3f} ({results.mean()*100:.1f}%)\")\n",
        "print(f\"   Std Deviation: {results.std():.3f} ({results.std()*100:.1f}%)\")\n",
        "print(f\"   Min Accuracy:  {results.min():.3f} ({results.min()*100:.1f}%)\")\n",
        "print(f\"   Max Accuracy:  {results.max():.3f} ({results.max()*100:.1f}%)\")\n",
        "\n",
        "# INTERPRETATION GUIDE\n",
        "print(\"\\nüí° What does this mean?\")\n",
        "if results.std() < 0.05:\n",
        "    print(\"   ‚úÖ Low standard deviation = Consistent performance!\")\n",
        "else:\n",
        "    print(\"   ‚ö†Ô∏è  High standard deviation = Performance varies a lot\")\n",
        "\n",
        "if results.mean() > 0.85:\n",
        "    print(\"   ‚úÖ High mean accuracy = Model performs well!\")\n",
        "elif results.mean() > 0.70:\n",
        "    print(\"   ‚ö†Ô∏è  Moderate accuracy = Room for improvement\")\n",
        "else:\n",
        "    print(\"   ‚ùå Low accuracy = Model needs work\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéì Understanding the Results\n",
        "\n",
        "**Mean Accuracy**: The average performance across all 5 folds\n",
        "- Think of it as your overall grade\n",
        "\n",
        "**Standard Deviation**: How much the scores vary\n",
        "- Low std dev (< 0.05): Your model is consistent! üéØ\n",
        "- High std dev (> 0.10): Your model's performance is unpredictable üé≤\n",
        "\n",
        "**Why 5 folds?**\n",
        "- Common choices: 5 or 10 folds\n",
        "- More folds = more training data per fold, but slower\n",
        "- Fewer folds = faster, but less reliable\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "understanding_kfold"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 2Ô∏è‚É£ Stratified K-Fold Cross-Validation\n",
        "\n",
        "## üéØ The \"Fair Distribution\" Technique\n",
        "\n",
        "### The Problem It Solves\n",
        "Imagine you have a class with:\n",
        "- 90 students who like pizza üçï\n",
        "- 10 students who like salad ü•ó\n",
        "\n",
        "If you randomly split into groups, one group might have NO salad lovers!\n",
        "\n",
        "### How Stratified K-Fold Helps\n",
        "It ensures each fold has the **same proportion** of each class:\n",
        "- Each fold will have ~90% pizza lovers and ~10% salad lovers\n",
        "\n",
        "### When to Use\n",
        "‚úÖ **Use when:** You have imbalanced classes (unequal numbers in categories)\n",
        "‚ùå **Don't use when:** You have regression problems (predicting numbers, not categories)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "stratified_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"STRATIFIED K-FOLD CROSS-VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# STEP 1: Create an IMBALANCED dataset\n",
        "# This simulates real-world scenarios like fraud detection\n",
        "# (where fraud cases are rare)\n",
        "X_imbalanced, y_imbalanced = make_classification(\n",
        "    n_samples=1000,\n",
        "    n_features=10,\n",
        "    n_classes=2,\n",
        "    weights=[0.9, 0.1],    # 90% class 0, 10% class 1\n",
        "    flip_y=0.01,           # Add 1% label noise\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# Check the imbalance\n",
        "class_counts = np.bincount(y_imbalanced)\n",
        "print(f\"\\nüìä Dataset Distribution:\")\n",
        "print(f\"   Class 0: {class_counts[0]} samples ({class_counts[0]/len(y_imbalanced)*100:.1f}%)\")\n",
        "print(f\"   Class 1: {class_counts[1]} samples ({class_counts[1]/len(y_imbalanced)*100:.1f}%)\")\n",
        "print(f\"   ‚ö†Ô∏è  This is IMBALANCED! Perfect for Stratified K-Fold.\")\n",
        "print()\n",
        "\n",
        "# STEP 2: Create model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# STEP 3: Compare Regular K-Fold vs Stratified K-Fold\n",
        "\n",
        "# Regular K-Fold (might create unfair splits)\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "regular_results = cross_val_score(model, X_imbalanced, y_imbalanced, cv=kfold)\n",
        "\n",
        "# Stratified K-Fold (ensures fair class distribution)\n",
        "stratified_kfold = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "stratified_results = cross_val_score(model, X_imbalanced, y_imbalanced, cv=stratified_kfold)\n",
        "\n",
        "# STEP 4: Compare results\n",
        "print(\"üîÑ Regular K-Fold Results:\")\n",
        "print(f\"   Scores: {[f'{s:.3f}' for s in regular_results]}\")\n",
        "print(f\"   Mean: {regular_results.mean():.3f}, Std: {regular_results.std():.3f}\")\n",
        "print()\n",
        "\n",
        "print(\"‚ú® Stratified K-Fold Results:\")\n",
        "print(f\"   Scores: {[f'{s:.3f}' for s in stratified_results]}\")\n",
        "print(f\"   Mean: {stratified_results.mean():.3f}, Std: {stratified_results.std():.3f}\")\n",
        "print()\n",
        "\n",
        "print(\"üìä Comparison:\")\n",
        "print(f\"   Difference in Std Dev: {abs(regular_results.std() - stratified_results.std()):.4f}\")\n",
        "if stratified_results.std() < regular_results.std():\n",
        "    print(\"   ‚úÖ Stratified K-Fold is MORE CONSISTENT!\")\n",
        "else:\n",
        "    print(\"   Similar consistency in this case\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "id": "stratified_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéì Why Use Stratified K-Fold?\n",
        "\n",
        "**Real-World Examples:**\n",
        "1. **Medical Diagnosis**: Rare diseases (few positive cases)\n",
        "2. **Fraud Detection**: Most transactions are legitimate\n",
        "3. **Spam Detection**: Most emails aren't spam\n",
        "\n",
        "**Key Benefit:**\n",
        "- Each fold has the same ratio of classes as the original dataset\n",
        "- More reliable performance estimates\n",
        "- Fairer comparison between models\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "stratified_explanation"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 3Ô∏è‚É£ Leave-One-Out Cross-Validation (LOO)\n",
        "\n",
        "## üéØ The \"Test Each Sample\" Technique\n",
        "\n",
        "### How It Works\n",
        "Imagine you have 100 data points:\n",
        "1. Use 1 point for testing, 99 for training\n",
        "2. Repeat 100 times, each time using a different point for testing\n",
        "3. Average all 100 results\n",
        "\n",
        "### Visual Representation\n",
        "```\n",
        "Iteration 1:  [TEST] [TRAIN] [TRAIN] ... [TRAIN]  (99 training points)\n",
        "Iteration 2:  [TRAIN] [TEST] [TRAIN] ... [TRAIN]\n",
        "Iteration 3:  [TRAIN] [TRAIN] [TEST] ... [TRAIN]\n",
        "...\n",
        "Iteration 100: [TRAIN] [TRAIN] [TRAIN] ... [TEST]\n",
        "```\n",
        "\n",
        "### Pros and Cons\n",
        "‚úÖ **Pros:**\n",
        "- Maximum use of data (99% for training each time!)\n",
        "- No randomness involved\n",
        "- Good for small datasets\n",
        "\n",
        "‚ùå **Cons:**\n",
        "- VERY slow (trains 100 models for 100 data points)\n",
        "- Computationally expensive\n",
        "- High variance in results\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "loo_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"LEAVE-ONE-OUT CROSS-VALIDATION\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# STEP 1: Create a SMALL dataset (LOO is slow, so we use small data)\n",
        "# Using only 50 samples to keep it fast\n",
        "X_small, y_small = make_classification(\n",
        "    n_samples=50,         # Small dataset\n",
        "    n_features=5,         # Fewer features\n",
        "    n_classes=2,\n",
        "    n_informative=3,\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "print(f\"\\nüìä Small Dataset: {X_small.shape[0]} samples\")\n",
        "print(f\"   ‚ö†Ô∏è  LOO will train {X_small.shape[0]} different models!\")\n",
        "print()\n",
        "\n",
        "# STEP 2: Create model\n",
        "model = LogisticRegression(max_iter=1000)\n",
        "\n",
        "# STEP 3: Set up Leave-One-Out\n",
        "loo = LeaveOneOut()\n",
        "\n",
        "# Count how many iterations we'll have\n",
        "n_iterations = loo.get_n_splits(X_small)\n",
        "print(f\"üîÑ Running {n_iterations} iterations...\")\n",
        "\n",
        "# STEP 4: Run LOO cross-validation\n",
        "import time\n",
        "start_time = time.time()\n",
        "\n",
        "loo_results = cross_val_score(model, X_small, y_small, cv=loo)\n",
        "\n",
        "end_time = time.time()\n",
        "elapsed_time = end_time - start_time\n",
        "\n",
        "# STEP 5: Display results\n",
        "print(f\"‚úÖ Completed in {elapsed_time:.2f} seconds\")\n",
        "print()\n",
        "\n",
        "# Show first 10 results (otherwise too many to display!)\n",
        "print(\"üìä First 10 Results (out of 50):\")\n",
        "print(f\"   {loo_results[:10]}\")\n",
        "print(f\"   ...\")\n",
        "print()\n",
        "\n",
        "print(\"üìà Summary Statistics:\")\n",
        "print(f\"   Mean Accuracy: {loo_results.mean():.3f} ({loo_results.mean()*100:.1f}%)\")\n",
        "print(f\"   Std Deviation: {loo_results.std():.3f}\")\n",
        "print(f\"   Correct Predictions: {np.sum(loo_results)} out of {len(loo_results)}\")\n",
        "print()\n",
        "\n",
        "# STEP 6: Compare with K-Fold\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "start_time = time.time()\n",
        "kfold_results = cross_val_score(model, X_small, y_small, cv=kfold)\n",
        "kfold_time = time.time() - start_time\n",
        "\n",
        "print(\"‚ö° Speed Comparison:\")\n",
        "print(f\"   LOO:    {elapsed_time:.3f} seconds ({n_iterations} models)\")\n",
        "print(f\"   K-Fold: {kfold_time:.3f} seconds (5 models)\")\n",
        "print(f\"   LOO is {elapsed_time/kfold_time:.1f}x SLOWER!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "id": "loo_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéì When to Use LOO?\n",
        "\n",
        "**Use LOO when:**\n",
        "- Dataset is VERY small (< 100 samples)\n",
        "- You need maximum use of training data\n",
        "- Computational time is not a concern\n",
        "\n",
        "**Avoid LOO when:**\n",
        "- Dataset is large (> 1000 samples)\n",
        "- You need fast results\n",
        "- K-Fold gives similar accuracy in less time\n",
        "\n",
        "**Pro Tip:** For most situations, **5-Fold or 10-Fold is better** than LOO!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "loo_explanation"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 4Ô∏è‚É£ Cross-Validation on Regression Problems\n",
        "\n",
        "## üè† Predicting Housing Prices\n",
        "\n",
        "So far we've done **classification** (predicting categories).\n",
        "Now let's try **regression** (predicting numbers)!\n",
        "\n",
        "### Real-World Example\n",
        "Predicting house prices based on:\n",
        "- Number of rooms üõèÔ∏è\n",
        "- Location üìç\n",
        "- Age of house üèöÔ∏è\n",
        "- Population density üë•\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "2M_JvFnFE6dj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"K-FOLD CROSS-VALIDATION ON LINEAR REGRESSION\")\n",
        "print(\"Predicting California Housing Prices\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# STEP 1: Load the California Housing dataset\n",
        "# This is real data about houses in California!\n",
        "housing = fetch_california_housing()\n",
        "\n",
        "print(\"\\nüìä Dataset Information:\")\n",
        "print(f\"   Samples: {housing.data.shape[0]}\")\n",
        "print(f\"   Features: {housing.data.shape[1]}\")\n",
        "print(f\"\\nüè† Features (what we measure):\")\n",
        "for i, feature in enumerate(housing.feature_names, 1):\n",
        "    print(f\"   {i}. {feature}\")\n",
        "print(f\"\\nüéØ Target: {housing.target_names[0]} (in $100,000s)\")\n",
        "print()\n",
        "\n",
        "# Prepare data\n",
        "X, y = housing.data, housing.target\n",
        "\n",
        "# STEP 2: Create Linear Regression model\n",
        "# This finds the best line to fit the data\n",
        "lin_reg = LinearRegression()\n",
        "\n",
        "# STEP 3: Set up K-Fold\n",
        "k = 5\n",
        "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "\n",
        "# STEP 4: Perform K-fold cross-validation MANUALLY\n",
        "# (to see what's happening behind the scenes)\n",
        "print(\"üîÑ Running K-Fold Cross-Validation...\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "kfold_scores = []\n",
        "for fold, (train_index, test_index) in enumerate(kfold.split(X), 1):\n",
        "    # Split data\n",
        "    X_train, X_test = X[train_index], X[test_index]\n",
        "    y_train, y_test = y[train_index], y[test_index]\n",
        "\n",
        "    # Train model\n",
        "    lin_reg.fit(X_train, y_train)\n",
        "\n",
        "    # Test model (R¬≤ score: 1.0 is perfect, 0.0 is random)\n",
        "    score = lin_reg.score(X_test, y_test)\n",
        "    kfold_scores.append(score)\n",
        "\n",
        "    print(f\"Fold {fold}: R¬≤ = {score:.4f}\")\n",
        "    print(f\"   Training samples: {len(X_train)}\")\n",
        "    print(f\"   Testing samples:  {len(X_test)}\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "\n",
        "# STEP 5: Calculate and display statistics\n",
        "mean_score = np.mean(kfold_scores)\n",
        "std_score = np.std(kfold_scores)\n",
        "\n",
        "print(f\"\\nüìä Final Results:\")\n",
        "print(f\"   Mean R¬≤ Score: {mean_score:.4f}\")\n",
        "print(f\"   Std Deviation: {std_score:.4f}\")\n",
        "print(f\"   Min Score: {min(kfold_scores):.4f}\")\n",
        "print(f\"   Max Score: {max(kfold_scores):.4f}\")\n",
        "\n",
        "# STEP 6: Interpret R¬≤ score\n",
        "print(f\"\\nüí° What does R¬≤ = {mean_score:.4f} mean?\")\n",
        "variance_explained = mean_score * 100\n",
        "print(f\"   The model explains {variance_explained:.1f}% of the variance in house prices\")\n",
        "\n",
        "if mean_score > 0.7:\n",
        "    print(\"   ‚úÖ Good! The model captures most of the patterns\")\n",
        "elif mean_score > 0.5:\n",
        "    print(\"   ‚ö†Ô∏è  Moderate. Some patterns are missed\")\n",
        "else:\n",
        "    print(\"   ‚ùå Poor. Model needs improvement\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "id": "_bm0EG_6FCpS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéì Understanding R¬≤ Score\n",
        "\n",
        "**What is R¬≤ (R-squared)?**\n",
        "- Measures how well your model predicts values\n",
        "- Range: 0.0 to 1.0 (can be negative for terrible models)\n",
        "\n",
        "**Interpretation Guide:**\n",
        "- **R¬≤ = 1.0**: Perfect predictions! üéØ\n",
        "- **R¬≤ = 0.8**: Explains 80% of variance (very good) ‚úÖ\n",
        "- **R¬≤ = 0.6**: Explains 60% of variance (okay) üëç\n",
        "- **R¬≤ = 0.3**: Explains 30% of variance (needs work) ‚ö†Ô∏è\n",
        "- **R¬≤ = 0.0**: Model is useless (just guessing the average) ‚ùå\n",
        "\n",
        "**Note:** For regression, we can't use Stratified K-Fold (that's only for classification!)\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "r2_explanation"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 5Ô∏è‚É£ Cross-Validation on Decision Trees\n",
        "\n",
        "## üå≥ Classification: Iris Flowers\n",
        "\n",
        "Decision Trees make decisions like a flowchart:\n",
        "- \"Is petal length > 2.5cm?\"\n",
        "  - If YES ‚Üí \"Is petal width > 1.7cm?\"\n",
        "  - If NO ‚Üí It's a Setosa!\n",
        "\n",
        "Let's classify iris flowers into 3 species! üå∏\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "gHBHYRy0L7F5"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"DECISION TREE CLASSIFICATION - IRIS FLOWERS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# STEP 1: Load the famous Iris dataset\n",
        "iris = load_iris()\n",
        "X, y = iris.data, iris.target\n",
        "\n",
        "print(\"\\nüå∏ Iris Dataset:\")\n",
        "print(f\"   Samples: {X.shape[0]}\")\n",
        "print(f\"   Features: {X.shape[1]}\")\n",
        "print(f\"\\nüìè Features:\")\n",
        "for i, feature in enumerate(iris.feature_names, 1):\n",
        "    print(f\"   {i}. {feature}\")\n",
        "\n",
        "print(f\"\\nüéØ Target Classes (Flower Species):\")\n",
        "for i, species in enumerate(iris.target_names):\n",
        "    count = np.sum(y == i)\n",
        "    print(f\"   {i}. {species}: {count} samples\")\n",
        "print()\n",
        "\n",
        "# STEP 2: Create Decision Tree model\n",
        "dtc = DecisionTreeClassifier(\n",
        "    max_depth=3,          # Limit tree depth to prevent overfitting\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "# STEP 3: Compare K-Fold vs Stratified K-Fold\n",
        "k = 5\n",
        "\n",
        "# Regular K-Fold\n",
        "kfold = KFold(n_splits=k, shuffle=True, random_state=42)\n",
        "kfold_scores = cross_val_score(dtc, X, y, cv=kfold)\n",
        "\n",
        "# Stratified K-Fold (better for multi-class classification)\n",
        "strat_kfold = StratifiedKFold(n_splits=k, shuffle=True, random_state=42)\n",
        "strat_scores = cross_val_score(dtc, X, y, cv=strat_kfold)\n",
        "\n",
        "# STEP 4: Display results side by side\n",
        "print(\"üìä Comparison of Cross-Validation Methods:\")\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Fold':<10} {'K-Fold':<15} {'Stratified K-Fold':<20}\")\n",
        "print(\"-\" * 50)\n",
        "\n",
        "for i in range(k):\n",
        "    print(f\"Fold {i+1:<5} {kfold_scores[i]:.3f} ({kfold_scores[i]*100:>5.1f}%)  \"\n",
        "          f\"{strat_scores[i]:.3f} ({strat_scores[i]*100:>5.1f}%)\")\n",
        "\n",
        "print(\"-\" * 50)\n",
        "print(f\"{'Mean':<10} {kfold_scores.mean():.3f} ({kfold_scores.mean()*100:>5.1f}%)  \"\n",
        "      f\"{strat_scores.mean():.3f} ({strat_scores.mean()*100:>5.1f}%)\")\n",
        "print(f\"{'Std Dev':<10} {kfold_scores.std():.3f} ({kfold_scores.std()*100:>5.1f}%)  \"\n",
        "      f\"{strat_scores.std():.3f} ({strat_scores.std()*100:>5.1f}%)\")\n",
        "\n",
        "print(\"\\nüí° Notice:\")\n",
        "if strat_scores.std() < kfold_scores.std():\n",
        "    print(\"   ‚úÖ Stratified K-Fold has LOWER variance (more consistent)\")\n",
        "    print(\"   This is because it maintains class balance in each fold!\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "id": "G9q2tY6oMFYu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 6Ô∏è‚É£ Time Series Cross-Validation\n",
        "\n",
        "## ‚è∞ The \"Respect Time Order\" Technique\n",
        "\n",
        "### Why Time Series is Different\n",
        "Imagine predicting tomorrow's weather:\n",
        "- ‚úÖ You CAN use yesterday's data to predict tomorrow\n",
        "- ‚ùå You CANNOT use tomorrow's data to predict yesterday!\n",
        "\n",
        "### The Problem with Regular K-Fold\n",
        "Regular K-Fold randomly shuffles data, which breaks time order:\n",
        "```\n",
        "‚ùå Wrong: Train on [Future] ‚Üí Test on [Past]\n",
        "```\n",
        "\n",
        "### Time Series Split Solution\n",
        "Always trains on past, tests on future:\n",
        "```\n",
        "Split 1: [Train ‚ñà‚ñà‚ñà‚ñà        ] [Test ‚ñà]\n",
        "Split 2: [Train ‚ñà‚ñà‚ñà‚ñà‚ñà       ] [Test ‚ñà]\n",
        "Split 3: [Train ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà      ] [Test ‚ñà]\n",
        "Split 4: [Train ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà     ] [Test ‚ñà]\n",
        "Split 5: [Train ‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà    ] [Test ‚ñà]\n",
        "         ‚Üê‚îÄ‚îÄ‚îÄ Past    Future ‚îÄ‚îÄ‚Üí\n",
        "```\n",
        "\n",
        "### Use Cases üìà\n",
        "- Stock prices prediction\n",
        "- Weather forecasting\n",
        "- Sales forecasting\n",
        "- Any data with time stamps!\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "joyCmWdEQKno"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"TIME SERIES CROSS-VALIDATION\")\n",
        "print(\"Forecasting Sales Over Time\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# STEP 1: Create a time series dataset\n",
        "# Simulating daily sales data with trend and seasonality\n",
        "dates = pd.date_range(start='2023-01-01', end='2023-12-31', freq='D')\n",
        "n_days = len(dates)\n",
        "\n",
        "# Create synthetic sales data with:\n",
        "# - Upward trend (business growing)\n",
        "# - Weekly seasonality (weekends are busier)\n",
        "# - Random noise\n",
        "trend = np.linspace(100, 150, n_days)  # Growing from 100 to 150\n",
        "seasonality = 20 * np.sin(2 * np.pi * np.arange(n_days) / 7)  # Weekly pattern\n",
        "noise = np.random.randn(n_days) * 5  # Random variation\n",
        "sales = trend + seasonality + noise\n",
        "\n",
        "# Create DataFrame\n",
        "data = pd.DataFrame({\n",
        "    'date': dates,\n",
        "    'sales': sales\n",
        "})\n",
        "data.set_index('date', inplace=True)\n",
        "\n",
        "print(f\"\\nüìÖ Time Period: {dates[0].date()} to {dates[-1].date()}\")\n",
        "print(f\"üìä Total Days: {n_days}\")\n",
        "print(f\"üí∞ Sales Range: ${sales.min():.2f} to ${sales.max():.2f}\")\n",
        "print()\n",
        "\n",
        "# STEP 2: Set up Time Series Cross-Validation\n",
        "n_splits = 5\n",
        "tscv = TimeSeriesSplit(n_splits=n_splits)\n",
        "\n",
        "print(f\"üîÑ Running Time Series Cross-Validation ({n_splits} splits)...\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# STEP 3: Visualize the splits\n",
        "print(f\"\\n{'Split':<8} {'Train Period':<25} {'Test Period':<25} {'Train Size':<12} {'Test Size'}\")\n",
        "print(\"-\" * 70)\n",
        "\n",
        "for i, (train_index, test_index) in enumerate(tscv.split(data), 1):\n",
        "    train_start = data.index[train_index[0]].date()\n",
        "    train_end = data.index[train_index[-1]].date()\n",
        "    test_start = data.index[test_index[0]].date()\n",
        "    test_end = data.index[test_index[-1]].date()\n",
        "\n",
        "    print(f\"Split {i}  {train_start} to {train_end}  \"\n",
        "          f\"{test_start} to {test_end}  \"\n",
        "          f\"{len(train_index):<12} {len(test_index)}\")\n",
        "\n",
        "print(\"-\" * 70)\n",
        "\n",
        "# STEP 4: Train and evaluate model on each split\n",
        "predictions_list = []\n",
        "true_labels_list = []\n",
        "scores = []\n",
        "\n",
        "model = LinearRegression()\n",
        "\n",
        "for fold, (train_index, test_index) in enumerate(tscv.split(data), 1):\n",
        "    # Split data\n",
        "    train_data = data.iloc[train_index]\n",
        "    test_data = data.iloc[test_index]\n",
        "\n",
        "    # Prepare features (using day number as feature)\n",
        "    X_train = train_data.index.astype('int64').values.reshape(-1, 1)\n",
        "    y_train = train_data['sales'].values\n",
        "    X_test = test_data.index.astype('int64').values.reshape(-1, 1)\n",
        "    y_test = test_data['sales'].values\n",
        "\n",
        "    # Train model\n",
        "    model.fit(X_train, y_train)\n",
        "\n",
        "    # Make predictions\n",
        "    y_pred = model.predict(X_test)\n",
        "\n",
        "    # Calculate R¬≤ score\n",
        "    score = model.score(X_test, y_test)\n",
        "    scores.append(score)\n",
        "\n",
        "    # Store for later\n",
        "    predictions_list.append(y_pred)\n",
        "    true_labels_list.append(y_test)\n",
        "\n",
        "# STEP 5: Calculate overall performance\n",
        "predictions = np.concatenate(predictions_list)\n",
        "true_labels = np.concatenate(true_labels_list)\n",
        "mse = ((predictions - true_labels) ** 2).mean()\n",
        "mae = np.abs(predictions - true_labels).mean()\n",
        "\n",
        "print(f\"\\nüìä Cross-Validation Results:\")\n",
        "print(f\"   Mean R¬≤ Score: {np.mean(scores):.4f}\")\n",
        "print(f\"   Std Deviation: {np.std(scores):.4f}\")\n",
        "print(f\"\\nüìà Error Metrics:\")\n",
        "print(f\"   Mean Squared Error: ${mse:.2f}\")\n",
        "print(f\"   Mean Absolute Error: ${mae:.2f}\")\n",
        "print(f\"   (On average, predictions are off by ${mae:.2f})\")\n",
        "\n",
        "print(\"\\nüí° Key Insight:\")\n",
        "print(\"   Notice how training size GROWS with each split!\")\n",
        "print(\"   This mimics real-world forecasting: more history = better predictions\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "id": "ztfFhknwQLUh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### üéì Time Series Best Practices\n",
        "\n",
        "**DO:**\n",
        "‚úÖ Use TimeSeriesSplit for any time-ordered data\n",
        "‚úÖ Keep data in chronological order\n",
        "‚úÖ Train on past, test on future\n",
        "‚úÖ Consider seasonality (daily, weekly, monthly patterns)\n",
        "\n",
        "**DON'T:**\n",
        "‚ùå Use regular K-Fold (it shuffles data!)\n",
        "‚ùå Use future data to predict the past\n",
        "‚ùå Shuffle time series data\n",
        "\n",
        "**Real-World Applications:**\n",
        "- üìà Stock market prediction\n",
        "- üå°Ô∏è Temperature forecasting\n",
        "- üè™ Retail sales forecasting\n",
        "- üìä Website traffic prediction\n",
        "- üí∞ Cryptocurrency price prediction\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "timeseries_explanation"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 7Ô∏è‚É£ Visualizing Cross-Validation Results\n",
        "\n",
        "## üìä Let's Make Beautiful Charts!\n",
        "\n",
        "Visualizations help us understand:\n",
        "- How consistent our model is\n",
        "- Which folds performed best/worst\n",
        "- How different CV methods compare\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "visualization_intro"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"=\"*60)\n",
        "print(\"VISUALIZING CROSS-VALIDATION RESULTS\")\n",
        "print(\"=\"*60)\n",
        "\n",
        "# STEP 1: Generate results from multiple CV methods\n",
        "X, y = load_iris(return_X_y=True)\n",
        "model = RandomForestClassifier(n_estimators=100, random_state=42)\n",
        "\n",
        "# Different CV methods\n",
        "cv_methods = {\n",
        "    '3-Fold': KFold(n_splits=3, shuffle=True, random_state=42),\n",
        "    '5-Fold': KFold(n_splits=5, shuffle=True, random_state=42),\n",
        "    '10-Fold': KFold(n_splits=10, shuffle=True, random_state=42),\n",
        "    'Stratified 5-Fold': StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "}\n",
        "\n",
        "results = {}\n",
        "for name, cv in cv_methods.items():\n",
        "    scores = cross_val_score(model, X, y, cv=cv)\n",
        "    results[name] = scores\n",
        "    print(f\"‚úÖ {name}: Mean = {scores.mean():.3f}, Std = {scores.std():.3f}\")\n",
        "\n",
        "print()\n",
        "\n",
        "# STEP 2: Create visualizations\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "fig.suptitle('Cross-Validation Results Comparison', fontsize=16, fontweight='bold')\n",
        "\n",
        "# Plot 1: Box Plot\n",
        "ax1 = axes[0, 0]\n",
        "ax1.boxplot(results.values(), labels=results.keys())\n",
        "ax1.set_title('Box Plot: Score Distribution', fontweight='bold')\n",
        "ax1.set_ylabel('Accuracy Score')\n",
        "ax1.set_ylim([0.85, 1.0])\n",
        "ax1.grid(True, alpha=0.3)\n",
        "ax1.tick_params(axis='x', rotation=45)\n",
        "\n",
        "# Plot 2: Bar Chart with Error Bars\n",
        "ax2 = axes[0, 1]\n",
        "means = [np.mean(scores) for scores in results.values()]\n",
        "stds = [np.std(scores) for scores in results.values()]\n",
        "x_pos = np.arange(len(results))\n",
        "ax2.bar(x_pos, means, yerr=stds, capsize=5, alpha=0.7, color='skyblue', edgecolor='navy')\n",
        "ax2.set_xticks(x_pos)\n",
        "ax2.set_xticklabels(results.keys(), rotation=45, ha='right')\n",
        "ax2.set_title('Mean Accuracy with Error Bars', fontweight='bold')\n",
        "ax2.set_ylabel('Mean Accuracy')\n",
        "ax2.set_ylim([0.85, 1.0])\n",
        "ax2.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "# Plot 3: Individual Fold Scores\n",
        "ax3 = axes[1, 0]\n",
        "for name, scores in results.items():\n",
        "    ax3.plot(range(1, len(scores) + 1), scores, marker='o', label=name, linewidth=2)\n",
        "ax3.set_title('Scores Across Folds', fontweight='bold')\n",
        "ax3.set_xlabel('Fold Number')\n",
        "ax3.set_ylabel('Accuracy Score')\n",
        "ax3.legend(loc='lower right')\n",
        "ax3.grid(True, alpha=0.3)\n",
        "ax3.set_ylim([0.85, 1.0])\n",
        "\n",
        "# Plot 4: Violin Plot\n",
        "ax4 = axes[1, 1]\n",
        "positions = range(1, len(results) + 1)\n",
        "parts = ax4.violinplot(results.values(), positions=positions, showmeans=True, showmedians=True)\n",
        "ax4.set_xticks(positions)\n",
        "ax4.set_xticklabels(results.keys(), rotation=45, ha='right')\n",
        "ax4.set_title('Violin Plot: Score Distribution', fontweight='bold')\n",
        "ax4.set_ylabel('Accuracy Score')\n",
        "ax4.set_ylim([0.85, 1.0])\n",
        "ax4.grid(True, alpha=0.3, axis='y')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Reading the Charts:\")\n",
        "print(\"   üì¶ Box Plot: Shows median, quartiles, and outliers\")\n",
        "print(\"   üìä Bar Chart: Shows average with error bars (¬±1 std dev)\")\n",
        "print(\"   üìà Line Plot: Shows how each fold performed\")\n",
        "print(\"   üéª Violin Plot: Shows full distribution shape\")\n",
        "\n",
        "print(\"\\n\" + \"=\"*60)"
      ],
      "metadata": {
        "id": "visualization_code"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 8Ô∏è‚É£ Best Practices & Common Pitfalls\n",
        "\n",
        "## ‚úÖ Best Practices\n",
        "\n",
        "### 1. Choosing the Right Number of Folds\n",
        "- **Small datasets (< 1000):** Use 5-10 folds\n",
        "- **Large datasets (> 10,000):** Use 3-5 folds\n",
        "- **Very small datasets (< 100):** Consider LOO\n",
        "\n",
        "### 2. Always Shuffle (Except Time Series!)\n",
        "```python\n",
        "# ‚úÖ Good\n",
        "KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "\n",
        "# ‚ùå Bad (unless data is pre-shuffled)\n",
        "KFold(n_splits=5, shuffle=False)\n",
        "```\n",
        "\n",
        "### 3. Use Stratified for Imbalanced Data\n",
        "```python\n",
        "# When class 1 is only 10% of data\n",
        "StratifiedKFold(n_splits=5)  # ‚úÖ Better\n",
        "```\n",
        "\n",
        "### 4. Set Random State for Reproducibility\n",
        "```python\n",
        "# ‚úÖ Results will be the same every time\n",
        "KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "```\n",
        "\n",
        "### 5. Look at Standard Deviation\n",
        "- Low std dev (< 0.05): Consistent model! üéØ\n",
        "- High std dev (> 0.10): Unstable model ‚ö†Ô∏è\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùå Common Pitfalls to Avoid\n",
        "\n",
        "### 1. Data Leakage\n",
        "```python\n",
        "# ‚ùå WRONG: Scaling before splitting\n",
        "X_scaled = scaler.fit_transform(X)  # Test data leaked into training!\n",
        "cross_val_score(model, X_scaled, y)\n",
        "\n",
        "# ‚úÖ CORRECT: Scale inside each fold\n",
        "# (We'll learn this in the Pipeline chapter)\n",
        "```\n",
        "\n",
        "### 2. Using K-Fold on Time Series\n",
        "```python\n",
        "# ‚ùå WRONG: Random splits break time order\n",
        "KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# ‚úÖ CORRECT: Use TimeSeriesSplit\n",
        "TimeSeriesSplit(n_splits=5)\n",
        "```\n",
        "\n",
        "### 3. Ignoring Class Imbalance\n",
        "```python\n",
        "# When you have: 90% class A, 10% class B\n",
        "\n",
        "# ‚ùå WRONG: Regular K-Fold\n",
        "KFold(n_splits=5)\n",
        "\n",
        "# ‚úÖ CORRECT: Stratified K-Fold\n",
        "StratifiedKFold(n_splits=5)\n",
        "```\n",
        "\n",
        "### 4. Too Many or Too Few Folds\n",
        "```python\n",
        "# ‚ùå Too few: Not reliable\n",
        "KFold(n_splits=2)\n",
        "\n",
        "# ‚ùå Too many: Slow and high variance\n",
        "KFold(n_splits=50)\n",
        "\n",
        "# ‚úÖ Just right: Standard choice\n",
        "KFold(n_splits=5)\n",
        "```\n",
        "\n",
        "### 5. Not Setting Random State\n",
        "```python\n",
        "# ‚ùå Results change every time\n",
        "KFold(n_splits=5, shuffle=True)\n",
        "\n",
        "# ‚úÖ Reproducible results\n",
        "KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "best_practices"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# 9Ô∏è‚É£ Quick Reference Guide\n",
        "\n",
        "## üéØ Decision Tree: Which CV Method Should I Use?\n",
        "\n",
        "```\n",
        "START\n",
        "  |\n",
        "  ‚îú‚îÄ Is it time series data?\n",
        "  ‚îÇ   ‚îú‚îÄ YES ‚Üí Use TimeSeriesSplit\n",
        "  ‚îÇ   ‚îî‚îÄ NO ‚Üì\n",
        "  |\n",
        "  ‚îú‚îÄ Is it classification or regression?\n",
        "  ‚îÇ   ‚îú‚îÄ REGRESSION ‚Üí Use KFold (5-10 folds)\n",
        "  ‚îÇ   ‚îî‚îÄ CLASSIFICATION ‚Üì\n",
        "  |\n",
        "  ‚îú‚îÄ Are classes balanced?\n",
        "  ‚îÇ   ‚îú‚îÄ YES ‚Üí Use KFold (5-10 folds)\n",
        "  ‚îÇ   ‚îî‚îÄ NO ‚Üí Use StratifiedKFold (5-10 folds)\n",
        "  |\n",
        "  ‚îú‚îÄ Is dataset very small (< 100 samples)?\n",
        "  ‚îÇ   ‚îú‚îÄ YES ‚Üí Consider LeaveOneOut\n",
        "  ‚îÇ   ‚îî‚îÄ NO ‚Üí Stick with K-Fold\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## üìã Cheat Sheet\n",
        "\n",
        "| Scenario | Best Method | Why? |\n",
        "|----------|-------------|------|\n",
        "| Balanced classification | K-Fold (5-10) | Fast and reliable |\n",
        "| Imbalanced classification | Stratified K-Fold | Preserves class ratios |\n",
        "| Regression | K-Fold (5-10) | Standard approach |\n",
        "| Time series | TimeSeriesSplit | Respects time order |\n",
        "| Very small dataset | LeaveOneOut | Maximizes training data |\n",
        "| Large dataset | K-Fold (3-5) | Faster computation |\n",
        "\n",
        "---\n",
        "\n",
        "## üíª Code Templates\n",
        "\n",
        "### Template 1: Basic K-Fold\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "model = LogisticRegression()\n",
        "kfold = KFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(model, X, y, cv=kfold)\n",
        "\n",
        "print(f\"Mean: {scores.mean():.3f}, Std: {scores.std():.3f}\")\n",
        "```\n",
        "\n",
        "### Template 2: Stratified K-Fold\n",
        "```python\n",
        "from sklearn.model_selection import cross_val_score, StratifiedKFold\n",
        "\n",
        "model = LogisticRegression()\n",
        "skf = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
        "scores = cross_val_score(model, X, y, cv=skf)\n",
        "\n",
        "print(f\"Mean: {scores.mean():.3f}, Std: {scores.std():.3f}\")\n",
        "```\n",
        "\n",
        "### Template 3: Time Series Split\n",
        "```python\n",
        "from sklearn.model_selection import TimeSeriesSplit\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "model = LinearRegression()\n",
        "tscv = TimeSeriesSplit(n_splits=5)\n",
        "\n",
        "for train_idx, test_idx in tscv.split(X):\n",
        "    X_train, X_test = X[train_idx], X[test_idx]\n",
        "    y_train, y_test = y[train_idx], y[test_idx]\n",
        "    model.fit(X_train, y_train)\n",
        "    score = model.score(X_test, y_test)\n",
        "    print(f\"Fold score: {score:.3f}\")\n",
        "```\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "quick_reference"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# üéì Practice Exercises\n",
        "\n",
        "## Exercise 1: Basic Cross-Validation\n",
        "**Task:** Load the Wine dataset and perform 5-fold cross-validation using a Decision Tree Classifier.\n",
        "\n",
        "**Your Goals:**\n",
        "1. Calculate mean accuracy\n",
        "2. Calculate standard deviation\n",
        "3. Determine if the model is consistent\n",
        "\n",
        "**Starter Code:**\n",
        "```python\n",
        "from sklearn.datasets import load_wine\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.model_selection import cross_val_score, KFold\n",
        "\n",
        "# Your code here!\n",
        "```\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 2: Compare CV Methods\n",
        "**Task:** Using the Iris dataset, compare:\n",
        "- 3-Fold CV\n",
        "- 5-Fold CV\n",
        "- 10-Fold CV\n",
        "- Stratified 5-Fold CV\n",
        "\n",
        "**Questions to Answer:**\n",
        "1. Which method gives the highest mean score?\n",
        "2. Which method has the lowest standard deviation?\n",
        "3. Which would you choose and why?\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 3: Time Series Challenge\n",
        "**Task:** Create a synthetic time series dataset and:\n",
        "1. Apply regular K-Fold (wrong approach)\n",
        "2. Apply TimeSeriesSplit (correct approach)\n",
        "3. Compare the results\n",
        "4. Explain why one is better than the other\n",
        "\n",
        "**Hint:** Create data with a clear trend!\n",
        "\n",
        "---\n",
        "\n",
        "## Exercise 4: Fix the Mistakes\n",
        "**Task:** This code has several cross-validation mistakes. Find and fix them!\n",
        "\n",
        "```python\n",
        "# Load data\n",
        "X, y = load_iris(return_X_y=True)\n",
        "\n",
        "# Create imbalanced dataset (90% class 0, 10% class 1)\n",
        "mask = (y == 0) | ((y == 1) & (np.random.rand(len(y)) < 0.1))\n",
        "X, y = X[mask], (y[mask] > 0).astype(int)\n",
        "\n",
        "# Cross-validation (FIND THE MISTAKES!)\n",
        "model = LogisticRegression()\n",
        "kfold = KFold(n_splits=2)  # Mistake 1?\n",
        "scores = cross_val_score(model, X, y, cv=kfold)  # Mistake 2?\n",
        "\n",
        "print(f\"Score: {scores[0]}\")  # Mistake 3?\n",
        "```\n",
        "\n",
        "**Questions:**\n",
        "1. What are the 3 mistakes?\n",
        "2. How would you fix them?\n",
        "3. Why are they mistakes?\n",
        "\n",
        "---"
      ],
      "metadata": {
        "id": "exercises"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Exercise Space - Write your solutions here!\n",
        "\n",
        "# Exercise 1: Your solution\n",
        "print(\"=\" * 60)\n",
        "print(\"EXERCISE 1 SOLUTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXERCISE 2 SOLUTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Your code here\n",
        "\n",
        "\n",
        "print(\"\\n\" + \"=\" * 60)\n",
        "print(\"EXERCISE 3 SOLUTION\")\n",
        "print(\"=\" * 60)\n",
        "\n",
        "# Your code here"
      ],
      "metadata": {
        "id": "exercise_space"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "---\n",
        "# üéâ Congratulations!\n",
        "\n",
        "## üìö What You've Learned\n",
        "\n",
        "You now understand:\n",
        "- ‚úÖ What cross-validation is and why it's important\n",
        "- ‚úÖ Different types of CV techniques (K-Fold, Stratified, LOO, Time Series)\n",
        "- ‚úÖ When to use each technique\n",
        "- ‚úÖ How to implement them in Python\n",
        "- ‚úÖ How to interpret CV results\n",
        "- ‚úÖ Common pitfalls and best practices\n",
        "\n",
        "---\n",
        "\n",
        "## üöÄ Next Steps\n",
        "\n",
        "1. **Practice** with different datasets\n",
        "2. **Experiment** with different numbers of folds\n",
        "3. **Combine** CV with hyperparameter tuning (coming soon!)\n",
        "4. **Apply** to your own projects\n",
        "\n",
        "---\n",
        "\n",
        "## üí° Remember\n",
        "\n",
        "> \"Cross-validation is like getting a second opinion, third opinion, fourth opinion... It makes you more confident in your model's performance!\"\n",
        "\n",
        "---\n",
        "\n",
        "## üìñ Additional Resources\n",
        "\n",
        "- [Scikit-learn Cross-Validation Guide](https://scikit-learn.org/stable/modules/cross_validation.html)\n",
        "- [StatQuest: Cross Validation](https://www.youtube.com/watch?v=fSytzGwwBVw)\n",
        "- Practice on [Kaggle](https://www.kaggle.com/)\n",
        "\n",
        "---\n",
        "\n",
        "## ‚ùì Questions?\n",
        "\n",
        "Contact: **Siva.Jasthi@metrostate.edu**\n",
        "\n",
        "---\n",
        "\n",
        "**Happy Learning! üéì**"
      ],
      "metadata": {
        "id": "conclusion"
      }
    }
  ]
}