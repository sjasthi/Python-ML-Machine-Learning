{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xdC_IPmUYeK9"
      },
      "source": [
        "# üé® Feature Engineering Playbook\n",
        "## Making Your Data Better for Machine Learning!\n",
        "\n",
        "### What is Feature Engineering? ü§î\n",
        "\n",
        "Imagine you're making a pizza. You have ingredients (flour, cheese, tomatoes), but you need to **prepare** them first:\n",
        "- Cut the tomatoes into slices\n",
        "- Grate the cheese\n",
        "- Mix and knead the dough\n",
        "\n",
        "**Feature Engineering** is like preparing ingredients for your ML model! We take raw data and transform it into features that help our model learn better.\n",
        "\n",
        "---\n",
        "\n",
        "### What You'll Learn Today:\n",
        "1. üîç Understanding Features\n",
        "2. üî¢ Handling Missing Data\n",
        "3. üè∑Ô∏è Encoding Categorical Variables\n",
        "4. üìè Feature Scaling\n",
        "5. ‚ú® Creating New Features\n",
        "6. üéØ Feature Selection\n",
        "\n",
        "Let's get started! üöÄ"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "u07tFtLBYeK-"
      },
      "outputs": [],
      "source": [
        "# Import our tools\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from sklearn.preprocessing import StandardScaler, MinMaxScaler, LabelEncoder, OneHotEncoder\n",
        "from sklearn.impute import SimpleImputer\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import PolynomialFeatures\n",
        "from sklearn.feature_selection import SelectKBest, f_classif\n",
        "\n",
        "# Make plots look nice\n",
        "plt.style.use('seaborn-v0_8-darkgrid')\n",
        "sns.set_palette(\"husl\")\n",
        "\n",
        "print(\"‚úÖ All libraries imported successfully!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "idlvXnarYeK-"
      },
      "source": [
        "---\n",
        "## 1. üîç Understanding Features\n",
        "\n",
        "**Features** are the characteristics we use to describe our data.\n",
        "\n",
        "**Example:** Describing a video game character:\n",
        "- **Height:** 180 cm (numerical)\n",
        "- **Strength:** 85/100 (numerical)\n",
        "- **Class:** Warrior (categorical)\n",
        "- **Has_Magic:** Yes (binary)\n",
        "\n",
        "Let's create a sample dataset about students!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Zb6Xy187YeK-"
      },
      "outputs": [],
      "source": [
        "# Creating a student dataset\n",
        "data = {\n",
        "    'Student_ID': range(1, 16),\n",
        "    'Study_Hours': [2, 4, 1, 5, 3, np.nan, 4, 2, 5, 3, 4, 1, np.nan, 3, 5],\n",
        "    'Sleep_Hours': [7, 8, 6, 7, np.nan, 8, 7, 6, 8, 7, 8, 5, 7, 6, 8],\n",
        "    'Favorite_Subject': ['Math', 'Science', 'Math', 'English', 'Science', 'Math',\n",
        "                         'English', 'Math', 'Science', 'English', 'Math', 'Science',\n",
        "                         'English', 'Math', 'Science'],\n",
        "    'Has_Pet': ['Yes', 'No', 'Yes', 'Yes', 'No', 'Yes', 'No', 'Yes', 'Yes', 'No',\n",
        "                'Yes', 'No', 'Yes', 'No', 'Yes'],\n",
        "    'Test_Score': [85, 92, 78, 95, 88, 82, 90, 80, 96, 87, 91, 75, 89, 83, 94]\n",
        "}\n",
        "\n",
        "df = pd.DataFrame(data)\n",
        "print(\"üìä Our Student Dataset:\")\n",
        "print(df)\n",
        "print(\"\\nüìà Dataset Info:\")\n",
        "print(df.info())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bXU1V0kCYeK-"
      },
      "source": [
        "---\n",
        "## 2. üî¢ Handling Missing Data\n",
        "\n",
        "Sometimes data is missing - like when a student forgets to fill in a survey question!\n",
        "\n",
        "**Common Strategies:**\n",
        "1. **Fill with mean** - Average value\n",
        "2. **Fill with median** - Middle value\n",
        "3. **Fill with mode** - Most common value\n",
        "4. **Remove the row** - Delete incomplete data\n",
        "\n",
        "Let's see the missing values in our data:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uSEIKGwMYeK-"
      },
      "outputs": [],
      "source": [
        "# Check for missing values\n",
        "print(\"‚ùå Missing Values:\")\n",
        "print(df.isnull().sum())\n",
        "\n",
        "# Visualize missing data\n",
        "plt.figure(figsize=(10, 4))\n",
        "sns.heatmap(df.isnull(), cbar=False, cmap='coolwarm', yticklabels=False)\n",
        "plt.title('Missing Data Visualization (Yellow = Missing)', fontsize=14, fontweight='bold')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7mdNUY4mYeK_"
      },
      "outputs": [],
      "source": [
        "# Strategy 1: Fill missing values with MEAN\n",
        "imputer_mean = SimpleImputer(strategy='mean')\n",
        "\n",
        "# Create a copy of our dataframe\n",
        "df_filled = df.copy()\n",
        "\n",
        "# Fill missing Study_Hours with mean\n",
        "df_filled[['Study_Hours']] = imputer_mean.fit_transform(df[['Study_Hours']])\n",
        "\n",
        "# Fill missing Sleep_Hours with mean\n",
        "df_filled[['Sleep_Hours']] = imputer_mean.fit_transform(df[['Sleep_Hours']])\n",
        "\n",
        "print(\"‚úÖ After filling missing values with MEAN:\")\n",
        "print(df_filled[['Study_Hours', 'Sleep_Hours']])\n",
        "\n",
        "print(\"\\nüéØ No more missing values!\")\n",
        "print(df_filled.isnull().sum())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MaiAcJ-pYeK_"
      },
      "source": [
        "### üéÆ Try It Yourself!\n",
        "\n",
        "**Challenge:** Try filling missing values with `median` instead of `mean`. What's the difference?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4x31aCADYeK_"
      },
      "outputs": [],
      "source": [
        "# YOUR TURN: Fill with median\n",
        "imputer_median = SimpleImputer(strategy='median')\n",
        "\n",
        "# TODO: Fill missing values with median\n",
        "# df_median = ...\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kUA3UTF-YeK_"
      },
      "source": [
        "---\n",
        "## 3. üè∑Ô∏è Encoding Categorical Variables\n",
        "\n",
        "Machine learning models speak the language of **numbers**, not words!\n",
        "\n",
        "We need to convert categories (like \"Math\", \"Science\") into numbers.\n",
        "\n",
        "### Two Main Techniques:\n",
        "\n",
        "#### A) **Label Encoding** - Assign numbers\n",
        "- Math ‚Üí 0\n",
        "- Science ‚Üí 1\n",
        "- English ‚Üí 2\n",
        "\n",
        "#### B) **One-Hot Encoding** - Create binary columns\n",
        "- Is_Math: 1 or 0\n",
        "- Is_Science: 1 or 0\n",
        "- Is_English: 1 or 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YRx8EuSpYeK_"
      },
      "outputs": [],
      "source": [
        "# Label Encoding for Has_Pet (Binary: Yes/No)\n",
        "label_encoder = LabelEncoder()\n",
        "df_filled['Has_Pet_Encoded'] = label_encoder.fit_transform(df_filled['Has_Pet'])\n",
        "\n",
        "print(\"üè∑Ô∏è Label Encoding for Has_Pet:\")\n",
        "print(df_filled[['Has_Pet', 'Has_Pet_Encoded']].head(10))\n",
        "print(\"\\nEncoding: No = 0, Yes = 1\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XfmbKcjGYeK_"
      },
      "outputs": [],
      "source": [
        "# One-Hot Encoding for Favorite_Subject\n",
        "subject_encoded = pd.get_dummies(df_filled['Favorite_Subject'], prefix='Subject')\n",
        "\n",
        "print(\"üéØ One-Hot Encoding for Favorite_Subject:\")\n",
        "print(subject_encoded.head(10))\n",
        "\n",
        "# Add to our dataframe\n",
        "df_filled = pd.concat([df_filled, subject_encoded], axis=1)\n",
        "print(\"\\n‚úÖ Updated DataFrame with encoded columns:\")\n",
        "print(df_filled.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sq6gjcIFYeK_"
      },
      "outputs": [],
      "source": [
        "# Visualize the encoding\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Original categories\n",
        "df_filled['Favorite_Subject'].value_counts().plot(kind='bar', ax=axes[0], color='skyblue')\n",
        "axes[0].set_title('Original Categories', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylabel('Count')\n",
        "axes[0].set_xlabel('Subject')\n",
        "\n",
        "# One-hot encoded\n",
        "subject_encoded.sum().plot(kind='bar', ax=axes[1], color='coral')\n",
        "axes[1].set_title('One-Hot Encoded Columns', fontsize=14, fontweight='bold')\n",
        "axes[1].set_ylabel('Count')\n",
        "axes[1].set_xlabel('Encoded Column')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "R18kqPjlYeK_"
      },
      "source": [
        "---\n",
        "## 4. üìè Feature Scaling\n",
        "\n",
        "Imagine comparing:\n",
        "- **Student's height:** 150 cm\n",
        "- **Student's age:** 13 years\n",
        "\n",
        "The numbers are on different scales! ML models get confused.\n",
        "\n",
        "**Solution:** Put all features on the same scale!\n",
        "\n",
        "### Two Popular Methods:\n",
        "\n",
        "#### 1) **Standardization (Z-score)** - Centers around 0\n",
        "Formula: `(x - mean) / standard_deviation`\n",
        "\n",
        "#### 2) **Normalization (Min-Max)** - Scales to 0-1\n",
        "Formula: `(x - min) / (max - min)`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eZtqW3w7YeK_"
      },
      "outputs": [],
      "source": [
        "# Let's look at our numerical features\n",
        "numerical_features = ['Study_Hours', 'Sleep_Hours', 'Test_Score']\n",
        "\n",
        "print(\"üìä Original Values:\")\n",
        "print(df_filled[numerical_features].describe())\n",
        "\n",
        "# Visualize original distributions\n",
        "fig, axes = plt.subplots(1, 3, figsize=(15, 4))\n",
        "for idx, col in enumerate(numerical_features):\n",
        "    axes[idx].hist(df_filled[col], bins=10, color='lightblue', edgecolor='black')\n",
        "    axes[idx].set_title(f'Original {col}', fontweight='bold')\n",
        "    axes[idx].set_xlabel(col)\n",
        "    axes[idx].set_ylabel('Frequency')\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dD67eNKbYeK_"
      },
      "outputs": [],
      "source": [
        "# Standardization (StandardScaler)\n",
        "scaler_standard = StandardScaler()\n",
        "df_standardized = df_filled.copy()\n",
        "df_standardized[numerical_features] = scaler_standard.fit_transform(df_filled[numerical_features])\n",
        "\n",
        "print(\"üìè After Standardization (mean=0, std=1):\")\n",
        "print(df_standardized[numerical_features].describe())\n",
        "\n",
        "# Normalization (MinMaxScaler)\n",
        "scaler_minmax = MinMaxScaler()\n",
        "df_normalized = df_filled.copy()\n",
        "df_normalized[numerical_features] = scaler_minmax.fit_transform(df_filled[numerical_features])\n",
        "\n",
        "print(\"\\nüéØ After Normalization (range: 0-1):\")\n",
        "print(df_normalized[numerical_features].describe())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Llnjo-TmYeK_"
      },
      "outputs": [],
      "source": [
        "# Compare all three side by side\n",
        "fig, axes = plt.subplots(3, 3, figsize=(15, 12))\n",
        "\n",
        "for idx, col in enumerate(numerical_features):\n",
        "    # Original\n",
        "    axes[0, idx].hist(df_filled[col], bins=10, color='lightblue', edgecolor='black')\n",
        "    axes[0, idx].set_title(f'Original {col}', fontweight='bold')\n",
        "\n",
        "    # Standardized\n",
        "    axes[1, idx].hist(df_standardized[col], bins=10, color='lightgreen', edgecolor='black')\n",
        "    axes[1, idx].set_title(f'Standardized {col}', fontweight='bold')\n",
        "\n",
        "    # Normalized\n",
        "    axes[2, idx].hist(df_normalized[col], bins=10, color='lightcoral', edgecolor='black')\n",
        "    axes[2, idx].set_title(f'Normalized {col}', fontweight='bold')\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5LKP8kHMYeK_"
      },
      "source": [
        "---\n",
        "## 5. ‚ú® Creating New Features\n",
        "\n",
        "Sometimes we can **create** better features by combining existing ones!\n",
        "\n",
        "**Examples:**\n",
        "- **Total_Hours** = Study_Hours + Sleep_Hours\n",
        "- **Study_Sleep_Ratio** = Study_Hours / Sleep_Hours\n",
        "- **Is_High_Scorer** = 1 if Test_Score > 90, else 0"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "qN-h2ZhCYeK_"
      },
      "outputs": [],
      "source": [
        "# Create new features\n",
        "df_features = df_filled.copy()\n",
        "\n",
        "# 1. Total Hours (combination)\n",
        "df_features['Total_Hours'] = df_features['Study_Hours'] + df_features['Sleep_Hours']\n",
        "\n",
        "# 2. Study-Sleep Ratio\n",
        "df_features['Study_Sleep_Ratio'] = df_features['Study_Hours'] / df_features['Sleep_Hours']\n",
        "\n",
        "# 3. Binary feature: Is High Scorer?\n",
        "df_features['Is_High_Scorer'] = (df_features['Test_Score'] >= 90).astype(int)\n",
        "\n",
        "# 4. Study Hours Squared (polynomial feature)\n",
        "df_features['Study_Hours_Squared'] = df_features['Study_Hours'] ** 2\n",
        "\n",
        "print(\"‚ú® New Features Created:\")\n",
        "print(df_features[['Study_Hours', 'Sleep_Hours', 'Total_Hours',\n",
        "                    'Study_Sleep_Ratio', 'Test_Score', 'Is_High_Scorer',\n",
        "                    'Study_Hours_Squared']].head(10))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "RU6MlD8tYeK_"
      },
      "outputs": [],
      "source": [
        "# Visualize the relationship between new features and test scores\n",
        "fig, axes = plt.subplots(2, 2, figsize=(14, 10))\n",
        "\n",
        "# Total Hours vs Test Score\n",
        "axes[0, 0].scatter(df_features['Total_Hours'], df_features['Test_Score'],\n",
        "                   c='blue', alpha=0.6, s=100)\n",
        "axes[0, 0].set_xlabel('Total Hours (Study + Sleep)', fontweight='bold')\n",
        "axes[0, 0].set_ylabel('Test Score', fontweight='bold')\n",
        "axes[0, 0].set_title('Total Hours vs Test Score', fontsize=12, fontweight='bold')\n",
        "axes[0, 0].grid(True, alpha=0.3)\n",
        "\n",
        "# Study-Sleep Ratio vs Test Score\n",
        "axes[0, 1].scatter(df_features['Study_Sleep_Ratio'], df_features['Test_Score'],\n",
        "                   c='green', alpha=0.6, s=100)\n",
        "axes[0, 1].set_xlabel('Study-Sleep Ratio', fontweight='bold')\n",
        "axes[0, 1].set_ylabel('Test Score', fontweight='bold')\n",
        "axes[0, 1].set_title('Study-Sleep Ratio vs Test Score', fontsize=12, fontweight='bold')\n",
        "axes[0, 1].grid(True, alpha=0.3)\n",
        "\n",
        "# High Scorers Distribution\n",
        "df_features['Is_High_Scorer'].value_counts().plot(kind='bar', ax=axes[1, 0],\n",
        "                                                    color=['coral', 'lightblue'])\n",
        "axes[1, 0].set_xlabel('Is High Scorer (0=No, 1=Yes)', fontweight='bold')\n",
        "axes[1, 0].set_ylabel('Count', fontweight='bold')\n",
        "axes[1, 0].set_title('Distribution of High Scorers', fontsize=12, fontweight='bold')\n",
        "axes[1, 0].set_xticklabels(['No (0)', 'Yes (1)'], rotation=0)\n",
        "\n",
        "# Study Hours Squared vs Test Score\n",
        "axes[1, 1].scatter(df_features['Study_Hours_Squared'], df_features['Test_Score'],\n",
        "                   c='purple', alpha=0.6, s=100)\n",
        "axes[1, 1].set_xlabel('Study Hours Squared', fontweight='bold')\n",
        "axes[1, 1].set_ylabel('Test Score', fontweight='bold')\n",
        "axes[1, 1].set_title('Study Hours¬≤ vs Test Score', fontsize=12, fontweight='bold')\n",
        "axes[1, 1].grid(True, alpha=0.3)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TYrby6IzYeLA"
      },
      "source": [
        "### üßÆ Polynomial Features\n",
        "\n",
        "Sometimes relationships aren't linear (straight lines)!\n",
        "\n",
        "Polynomial features help capture **curved** relationships."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tB6g7MpPYeLA"
      },
      "outputs": [],
      "source": [
        "# Create polynomial features\n",
        "poly = PolynomialFeatures(degree=2, include_bias=False)\n",
        "\n",
        "# Use just Study_Hours and Sleep_Hours\n",
        "features_for_poly = df_filled[['Study_Hours', 'Sleep_Hours']]\n",
        "poly_features = poly.fit_transform(features_for_poly)\n",
        "\n",
        "# Get feature names\n",
        "poly_feature_names = poly.get_feature_names_out(['Study_Hours', 'Sleep_Hours'])\n",
        "\n",
        "print(\"üßÆ Polynomial Features (degree=2):\")\n",
        "print(f\"Original features: {list(features_for_poly.columns)}\")\n",
        "print(f\"\\nNew polynomial features: {list(poly_feature_names)}\")\n",
        "print(f\"\\nNumber of features increased from {features_for_poly.shape[1]} to {poly_features.shape[1]}!\")\n",
        "\n",
        "# Show first few rows\n",
        "poly_df = pd.DataFrame(poly_features, columns=poly_feature_names)\n",
        "print(\"\\nüìä Sample Polynomial Features:\")\n",
        "print(poly_df.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ie9sD0vvYeLA"
      },
      "source": [
        "---\n",
        "## 6. üéØ Feature Selection\n",
        "\n",
        "**Too many features?** Not all features are helpful!\n",
        "\n",
        "Feature selection helps us pick the **most important** features.\n",
        "\n",
        "**Why?**\n",
        "- Faster training\n",
        "- Better performance\n",
        "- Easier to understand\n",
        "\n",
        "Let's find which features best predict test scores!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DrZykDFrYeLA"
      },
      "outputs": [],
      "source": [
        "# Prepare features for selection\n",
        "# We'll use numerical and encoded features\n",
        "feature_columns = ['Study_Hours', 'Sleep_Hours', 'Has_Pet_Encoded',\n",
        "                   'Subject_English', 'Subject_Math', 'Subject_Science']\n",
        "\n",
        "X = df_filled[feature_columns]\n",
        "y = df_filled['Test_Score']\n",
        "\n",
        "print(\"üìã Our Features:\")\n",
        "print(X.head())\n",
        "print(\"\\nüéØ Target (What we're predicting):\")\n",
        "print(y.head())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ONlNPUk5YeLA"
      },
      "outputs": [],
      "source": [
        "# Convert to binary classification for feature selection\n",
        "# High scorer = 1, Not high scorer = 0\n",
        "y_binary = (y >= 90).astype(int)\n",
        "\n",
        "# Select K Best features using f_classif\n",
        "selector = SelectKBest(score_func=f_classif, k=3)  # Select top 3 features\n",
        "X_selected = selector.fit_transform(X, y_binary)\n",
        "\n",
        "# Get feature scores\n",
        "feature_scores = pd.DataFrame({\n",
        "    'Feature': feature_columns,\n",
        "    'Score': selector.scores_\n",
        "}).sort_values('Score', ascending=False)\n",
        "\n",
        "print(\"üèÜ Feature Importance Scores:\")\n",
        "print(feature_scores)\n",
        "\n",
        "# Get selected features\n",
        "selected_features = [feature_columns[i] for i in selector.get_support(indices=True)]\n",
        "print(f\"\\n‚úÖ Top {selector.k} Selected Features: {selected_features}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AS9pIhZdYeLA"
      },
      "outputs": [],
      "source": [
        "# Visualize feature importance\n",
        "plt.figure(figsize=(12, 6))\n",
        "colors = ['gold' if i < 3 else 'lightgray' for i in range(len(feature_scores))]\n",
        "plt.barh(feature_scores['Feature'], feature_scores['Score'], color=colors)\n",
        "plt.xlabel('Importance Score', fontsize=12, fontweight='bold')\n",
        "plt.ylabel('Features', fontsize=12, fontweight='bold')\n",
        "plt.title('Feature Importance for Predicting High Test Scores',\n",
        "          fontsize=14, fontweight='bold')\n",
        "plt.axvline(x=feature_scores['Score'].iloc[2], color='red',\n",
        "            linestyle='--', label='Selection Threshold')\n",
        "plt.legend()\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüí° Gold bars = Selected features!\")\n",
        "print(\"Gray bars = Not selected (less important)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e1Qxe5zAYeLA"
      },
      "source": [
        "---\n",
        "## 7. ü§ñ Putting It All Together: Build a Model!\n",
        "\n",
        "Let's see how feature engineering improves our model!\n",
        "\n",
        "We'll compare:\n",
        "1. **Raw data** (no feature engineering)\n",
        "2. **Engineered features** (with all our transformations)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "szufl-jsYeLA"
      },
      "outputs": [],
      "source": [
        "# Prepare data for modeling\n",
        "# Model 1: Using RAW features (just Study_Hours and Sleep_Hours)\n",
        "X_raw = df_filled[['Study_Hours', 'Sleep_Hours']].copy()\n",
        "y_target = (df_filled['Test_Score'] >= 90).astype(int)  # Binary: High scorer or not\n",
        "\n",
        "# Split data\n",
        "X_train_raw, X_test_raw, y_train, y_test = train_test_split(\n",
        "    X_raw, y_target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train model with raw features\n",
        "model_raw = LogisticRegression(random_state=42)\n",
        "model_raw.fit(X_train_raw, y_train)\n",
        "y_pred_raw = model_raw.predict(X_test_raw)\n",
        "accuracy_raw = accuracy_score(y_test, y_pred_raw)\n",
        "\n",
        "print(\"üìä Model 1: Using RAW Features\")\n",
        "print(f\"Features used: {list(X_raw.columns)}\")\n",
        "print(f\"Accuracy: {accuracy_raw:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "KbDf154iYeLA"
      },
      "outputs": [],
      "source": [
        "# Model 2: Using ENGINEERED features\n",
        "# Create engineered features\n",
        "X_engineered = df_filled[['Study_Hours', 'Sleep_Hours']].copy()\n",
        "\n",
        "# Add new features\n",
        "X_engineered['Total_Hours'] = X_engineered['Study_Hours'] + X_engineered['Sleep_Hours']\n",
        "X_engineered['Study_Sleep_Ratio'] = X_engineered['Study_Hours'] / X_engineered['Sleep_Hours']\n",
        "X_engineered['Study_Hours_Squared'] = X_engineered['Study_Hours'] ** 2\n",
        "\n",
        "# Add encoded categorical features\n",
        "X_engineered['Has_Pet'] = df_filled['Has_Pet_Encoded']\n",
        "X_engineered = pd.concat([X_engineered, subject_encoded], axis=1)\n",
        "\n",
        "# Scale features\n",
        "scaler = StandardScaler()\n",
        "X_engineered_scaled = pd.DataFrame(\n",
        "    scaler.fit_transform(X_engineered),\n",
        "    columns=X_engineered.columns\n",
        ")\n",
        "\n",
        "# Split data\n",
        "X_train_eng, X_test_eng, y_train_eng, y_test_eng = train_test_split(\n",
        "    X_engineered_scaled, y_target, test_size=0.3, random_state=42\n",
        ")\n",
        "\n",
        "# Train model with engineered features\n",
        "model_engineered = LogisticRegression(random_state=42, max_iter=1000)\n",
        "model_engineered.fit(X_train_eng, y_train_eng)\n",
        "y_pred_eng = model_engineered.predict(X_test_eng)\n",
        "accuracy_eng = accuracy_score(y_test_eng, y_pred_eng)\n",
        "\n",
        "print(\"\\n‚ú® Model 2: Using ENGINEERED Features\")\n",
        "print(f\"Features used: {list(X_engineered.columns)}\")\n",
        "print(f\"Accuracy: {accuracy_eng:.2%}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bz-7oro3YeLA"
      },
      "outputs": [],
      "source": [
        "# Compare the two models\n",
        "comparison = pd.DataFrame({\n",
        "    'Model': ['Raw Features', 'Engineered Features'],\n",
        "    'Number of Features': [X_raw.shape[1], X_engineered.shape[1]],\n",
        "    'Accuracy': [accuracy_raw, accuracy_eng],\n",
        "    'Improvement': [0, accuracy_eng - accuracy_raw]\n",
        "})\n",
        "\n",
        "print(\"\\nüìä Model Comparison:\")\n",
        "print(comparison)\n",
        "\n",
        "# Visualize comparison\n",
        "fig, axes = plt.subplots(1, 2, figsize=(14, 5))\n",
        "\n",
        "# Accuracy comparison\n",
        "models = ['Raw Features', 'Engineered Features']\n",
        "accuracies = [accuracy_raw, accuracy_eng]\n",
        "colors_bar = ['lightcoral', 'lightgreen']\n",
        "\n",
        "axes[0].bar(models, accuracies, color=colors_bar, edgecolor='black', linewidth=2)\n",
        "axes[0].set_ylabel('Accuracy', fontsize=12, fontweight='bold')\n",
        "axes[0].set_title('Model Performance Comparison', fontsize=14, fontweight='bold')\n",
        "axes[0].set_ylim([0, 1.1])\n",
        "for i, v in enumerate(accuracies):\n",
        "    axes[0].text(i, v + 0.02, f'{v:.2%}', ha='center', fontweight='bold', fontsize=12)\n",
        "\n",
        "# Feature count comparison\n",
        "feature_counts = [X_raw.shape[1], X_engineered.shape[1]]\n",
        "axes[1].bar(models, feature_counts, color=colors_bar, edgecolor='black', linewidth=2)\n",
        "axes[1].set_ylabel('Number of Features', fontsize=12, fontweight='bold')\n",
        "axes[1].set_title('Feature Count Comparison', fontsize=14, fontweight='bold')\n",
        "for i, v in enumerate(feature_counts):\n",
        "    axes[1].text(i, v + 0.2, str(v), ha='center', fontweight='bold', fontsize=12)\n",
        "\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "print(\"\\nüéâ Feature Engineering can improve model performance!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DzpFoJUNYeLA"
      },
      "source": [
        "---\n",
        "## 8. üéì Summary: Your Feature Engineering Toolkit\n",
        "\n",
        "### What We Learned:\n",
        "\n",
        "| Technique | What It Does | When To Use |\n",
        "|-----------|--------------|-------------|\n",
        "| **Handling Missing Data** | Fill or remove missing values | When you have incomplete data |\n",
        "| **Label Encoding** | Convert categories to numbers | For binary or ordinal categories |\n",
        "| **One-Hot Encoding** | Create binary columns for categories | For nominal categories |\n",
        "| **Standardization** | Scale features to mean=0, std=1 | When features have different units |\n",
        "| **Normalization** | Scale features to 0-1 range | When you want bounded values |\n",
        "| **Feature Creation** | Combine or transform features | To capture new patterns |\n",
        "| **Polynomial Features** | Create interaction terms | For non-linear relationships |\n",
        "| **Feature Selection** | Pick most important features | To reduce complexity |\n",
        "\n",
        "### üîë Key Takeaways:\n",
        "\n",
        "1. **Feature Engineering is an art AND a science** - experiment with different transformations!\n",
        "2. **More features ‚â† Better model** - quality over quantity!\n",
        "3. **Always scale your features** when using distance-based algorithms\n",
        "4. **Understand your data first** - look at distributions, missing values, correlations\n",
        "5. **Test different approaches** - compare raw vs engineered features\n",
        "\n",
        "### üöÄ Next Steps:\n",
        "\n",
        "1. Try these techniques on your own datasets!\n",
        "2. Experiment with creating domain-specific features\n",
        "3. Learn about advanced techniques like:\n",
        "   - Target encoding\n",
        "   - Feature hashing\n",
        "   - Dimensionality reduction (PCA)\n",
        "   - Automatic feature engineering tools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2n8JvUARYeLA"
      },
      "source": [
        "---\n",
        "## üéÆ Practice Challenges\n",
        "\n",
        "### Challenge 1: Create Your Own Features\n",
        "Create 3 new features from the student dataset and test if they improve the model!\n",
        "\n",
        "### Challenge 2: Different Scalers\n",
        "Compare StandardScaler vs MinMaxScaler vs RobustScaler. Which works best?\n",
        "\n",
        "### Challenge 3: Feature Engineering Pipeline\n",
        "Build a complete feature engineering pipeline that:\n",
        "1. Handles missing values\n",
        "2. Encodes categories\n",
        "3. Creates new features\n",
        "4. Scales data\n",
        "5. Selects best features\n",
        "\n",
        "### Challenge 4: Real Dataset\n",
        "Apply these techniques to a real dataset (like Titanic, House Prices, or Iris)!"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HjBr5bQ1YeLA"
      },
      "outputs": [],
      "source": [
        "# YOUR PRACTICE SPACE - Try the challenges here!\n",
        "\n",
        "# Challenge 1: Create your own features\n",
        "\n",
        "\n",
        "# Challenge 2: Compare different scalers\n",
        "\n",
        "\n",
        "# Challenge 3: Build a complete pipeline\n",
        "\n",
        "\n",
        "# Challenge 4: Try on a real dataset\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "FQqoDMtKYeLA"
      },
      "source": [
        "---\n",
        "## üìö Additional Resources\n",
        "\n",
        "### Learn More:\n",
        "- [Scikit-learn Preprocessing Guide](https://scikit-learn.org/stable/modules/preprocessing.html)\n",
        "- [Feature Engineering for Machine Learning](https://www.kaggle.com/learn/feature-engineering)\n",
        "- [Pandas Documentation](https://pandas.pydata.org/docs/)\n",
        "\n",
        "### Practice Datasets:\n",
        "- [Kaggle Datasets](https://www.kaggle.com/datasets)\n",
        "- [UCI Machine Learning Repository](https://archive.ics.uci.edu/ml/index.php)\n",
        "- [Scikit-learn Toy Datasets](https://scikit-learn.org/stable/datasets/toy_dataset.html)\n",
        "\n",
        "---\n",
        "\n",
        "## üéâ Congratulations!\n",
        "\n",
        "You've completed the Feature Engineering Playbook! You now know how to:\n",
        "- ‚úÖ Handle missing data\n",
        "- ‚úÖ Encode categorical variables\n",
        "- ‚úÖ Scale features\n",
        "- ‚úÖ Create new features\n",
        "- ‚úÖ Select important features\n",
        "- ‚úÖ Build better ML models\n",
        "\n",
        "**Keep practicing and experimenting! üöÄ**\n",
        "\n",
        "---\n",
        "\n",
        "*Created with ‚ù§Ô∏è for middle school ML students*"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.0"
    },
    "colab": {
      "provenance": [],
      "toc_visible": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}