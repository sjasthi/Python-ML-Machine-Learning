{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hierarchical Clustering ðŸŒ³\n",
    "## Building Family Trees of Data!\n",
    "\n",
    "**Course:** Python for Machine Learning  \n",
    "**Topic:** Hierarchical Clustering  \n",
    "**Week:** 13 (Part 1)\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸ“š What You'll Learn\n",
    "\n",
    "In this notebook, you'll learn:\n",
    "\n",
    "1. âœ… What hierarchical clustering is and when to use it\n",
    "2. âœ… The difference between agglomerative (bottom-up) and divisive (top-down)\n",
    "3. âœ… How to measure distance between clusters (linkage methods)\n",
    "4. âœ… How to create and interpret dendrograms\n",
    "5. âœ… Implementation from scratch and with scikit-learn\n",
    "6. âœ… Choosing the optimal number of clusters\n",
    "7. âœ… When to use hierarchical clustering vs. K-Means\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸŽ¯ Setup: Import Libraries\n",
    "\n",
    "Let's import everything we'll need:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data manipulation\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Visualization\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Hierarchical clustering\n",
    "from scipy.cluster.hierarchy import dendrogram, linkage, fcluster\n",
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# For comparison\n",
    "from sklearn.cluster import KMeans\n",
    "\n",
    "# Preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "# Sample datasets\n",
    "from sklearn.datasets import make_blobs, make_moons\n",
    "\n",
    "# Set style\n",
    "plt.style.use('seaborn-v0_8-darkgrid')\n",
    "sns.set_palette(\"husl\")\n",
    "\n",
    "print(\"âœ… All libraries imported successfully!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 1: Introduction to Hierarchical Clustering ðŸŒ³\n",
    "\n",
    "## ðŸ¤” What is Hierarchical Clustering?\n",
    "\n",
    "Imagine organizing all living things on Earth into groups:\n",
    "- **Option 1:** Start with individual animals â†’ group similar ones â†’ keep grouping\n",
    "- **Option 2:** Start with \"all animals\" â†’ split into groups â†’ keep splitting\n",
    "\n",
    "Both approaches create a **hierarchy** - a family tree showing relationships!\n",
    "\n",
    "### Key Differences from K-Means:\n",
    "\n",
    "| Feature | K-Means | Hierarchical |\n",
    "|---------|---------|-------------|\n",
    "| Need to specify K? | âœ“ Yes | âœ— No (can decide later!) |\n",
    "| Creates hierarchy? | âœ— No | âœ“ Yes (dendrogram) |\n",
    "| Speed | âš¡ Fast | ðŸŒ Slower |\n",
    "| Best for | Large datasets | Understanding structure |\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Create Sample Data\n",
    "\n",
    "Let's create student study habits data:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Student study data: [study_hours_per_week, test_score]\n",
    "students = {\n",
    "    'Alice': [2, 60],\n",
    "    'Bob': [2.5, 65],\n",
    "    'Charlie': [8, 90],\n",
    "    'Diana': [8.5, 92],\n",
    "    'Eve': [5, 75],\n",
    "    'Frank': [5.5, 78],\n",
    "    'Grace': [3, 68],\n",
    "    'Henry': [9, 95]\n",
    "}\n",
    "\n",
    "# Convert to arrays\n",
    "student_names = list(students.keys())\n",
    "X = np.array(list(students.values()))\n",
    "\n",
    "# Create DataFrame for easy viewing\n",
    "df = pd.DataFrame(X, columns=['Study Hours/Week', 'Test Score'], index=student_names)\n",
    "print(\"ðŸ“š Student Study Habits Data:\")\n",
    "print(df)\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.scatter(X[:, 0], X[:, 1], s=300, alpha=0.6, edgecolors='black', linewidth=2)\n",
    "\n",
    "# Add labels\n",
    "for i, name in enumerate(student_names):\n",
    "    plt.annotate(name, (X[i, 0], X[i, 1]), fontsize=11, ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Study Hours per Week', fontsize=12)\n",
    "plt.ylabel('Test Score', fontsize=12)\n",
    "plt.title('Student Study Habits ðŸ“–', fontsize=14, fontweight='bold')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ‘€ Can you visually identify natural groups?\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 2: Agglomerative Clustering (Bottom-Up) ðŸŒ±\n",
    "\n",
    "## The Algorithm:\n",
    "\n",
    "**Step 1:** Start with each point as its own cluster  \n",
    "**Step 2:** Find the two closest clusters  \n",
    "**Step 3:** Merge them into one cluster  \n",
    "**Step 4:** Repeat until all points are in one cluster  \n",
    "\n",
    "It's like building with LEGO blocks - start with individual pieces and snap them together!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ’» Build from Scratch: Simple Implementation\n",
    "\n",
    "Let's implement a simple version to understand the process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import math\n",
    "\n",
    "def euclidean_distance(p1, p2):\n",
    "    \"\"\"Calculate distance between two points\"\"\"\n",
    "    return math.sqrt((p1[0] - p2[0])**2 + (p1[1] - p2[1])**2)\n",
    "\n",
    "def cluster_distance_single(cluster1, cluster2):\n",
    "    \"\"\"Single linkage: minimum distance between any two points\"\"\"\n",
    "    min_dist = float('inf')\n",
    "    for p1 in cluster1:\n",
    "        for p2 in cluster2:\n",
    "            dist = euclidean_distance(p1, p2)\n",
    "            min_dist = min(min_dist, dist)\n",
    "    return min_dist\n",
    "\n",
    "def simple_hierarchical_clustering(data, names):\n",
    "    \"\"\"Simple agglomerative clustering implementation\"\"\"\n",
    "    # Start: each point is its own cluster\n",
    "    clusters = [[i] for i in range(len(data))]\n",
    "    cluster_data = [[data[i]] for i in range(len(data))]\n",
    "    \n",
    "    step = 0\n",
    "    print(\"ðŸŒ± Starting Agglomerative Clustering (Bottom-Up)\")\n",
    "    print(\"=\"*60)\n",
    "    print(f\"\\nStep {step}: {len(clusters)} clusters (each student separate)\")\n",
    "    for i, cluster in enumerate(clusters):\n",
    "        print(f\"  Cluster {i}: {[names[idx] for idx in cluster]}\")\n",
    "    \n",
    "    # Keep merging until we have one cluster\n",
    "    while len(clusters) > 1:\n",
    "        step += 1\n",
    "        \n",
    "        # Find two closest clusters\n",
    "        min_dist = float('inf')\n",
    "        merge_i, merge_j = 0, 1\n",
    "        \n",
    "        for i in range(len(clusters)):\n",
    "            for j in range(i + 1, len(clusters)):\n",
    "                dist = cluster_distance_single(cluster_data[i], cluster_data[j])\n",
    "                if dist < min_dist:\n",
    "                    min_dist = dist\n",
    "                    merge_i, merge_j = i, j\n",
    "        \n",
    "        # Merge the two closest clusters\n",
    "        print(f\"\\nStep {step}: Merging clusters {merge_i} and {merge_j}\")\n",
    "        print(f\"  Distance: {min_dist:.2f}\")\n",
    "        print(f\"  {[names[idx] for idx in clusters[merge_i]]} + {[names[idx] for idx in clusters[merge_j]]}\")\n",
    "        \n",
    "        # Create merged cluster\n",
    "        merged_indices = clusters[merge_i] + clusters[merge_j]\n",
    "        merged_data = cluster_data[merge_i] + cluster_data[merge_j]\n",
    "        \n",
    "        # Remove old clusters and add merged one\n",
    "        new_clusters = [c for idx, c in enumerate(clusters) if idx not in [merge_i, merge_j]]\n",
    "        new_data = [c for idx, c in enumerate(cluster_data) if idx not in [merge_i, merge_j]]\n",
    "        \n",
    "        new_clusters.append(merged_indices)\n",
    "        new_data.append(merged_data)\n",
    "        \n",
    "        clusters = new_clusters\n",
    "        cluster_data = new_data\n",
    "        \n",
    "        # Show current state\n",
    "        print(f\"  Now we have {len(clusters)} cluster(s):\")\n",
    "        for i, cluster in enumerate(clusters):\n",
    "            print(f\"    Cluster {i}: {[names[idx] for idx in cluster]}\")\n",
    "    \n",
    "    print(\"\\n\" + \"=\"*60)\n",
    "    print(\"âœ… Done! All students merged into one cluster!\")\n",
    "\n",
    "# Run the simple implementation\n",
    "simple_hierarchical_clustering(X, student_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Key Observation:\n",
    "\n",
    "Notice how students with similar study habits merged first:  \n",
    "- Alice & Bob (both low study hours)  \n",
    "- Charlie & Diana (both high study hours)  \n",
    "- Eve & Frank (medium study hours)\n",
    "\n",
    "This creates a natural hierarchy!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 3: Linkage Methods ðŸ”—\n",
    "\n",
    "How do we measure distance between **clusters** (not just points)?\n",
    "\n",
    "## Four Main Methods:\n",
    "\n",
    "### 1. Single Linkage (Nearest Point)\n",
    "Distance = **minimum** distance between any two points\n",
    "- âœ… Pros: Can find chain-like patterns\n",
    "- âŒ Cons: Sensitive to noise, creates long chains\n",
    "\n",
    "### 2. Complete Linkage (Farthest Point)\n",
    "Distance = **maximum** distance between any two points\n",
    "- âœ… Pros: Creates compact clusters\n",
    "- âŒ Cons: Sensitive to outliers\n",
    "\n",
    "### 3. Average Linkage\n",
    "Distance = **average** of all pairwise distances\n",
    "- âœ… Pros: Balanced, usually works well\n",
    "- âŒ Cons: Moderate computational cost\n",
    "\n",
    "### 4. Ward's Method\n",
    "Distance = increase in **variance** after merging\n",
    "- âœ… Pros: Creates balanced, similar-sized clusters\n",
    "- âŒ Cons: Assumes spherical clusters (like K-Means)\n",
    "\n",
    "Let's visualize the differences!\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Compare All Linkage Methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Test all linkage methods\n",
    "linkage_methods = ['single', 'complete', 'average', 'ward']\n",
    "\n",
    "fig, axes = plt.subplots(2, 2, figsize=(16, 12))\n",
    "axes = axes.flatten()\n",
    "\n",
    "for idx, method in enumerate(linkage_methods):\n",
    "    # Perform hierarchical clustering\n",
    "    Z = linkage(X, method=method)\n",
    "    \n",
    "    # Create dendrogram\n",
    "    dendrogram(Z, labels=student_names, ax=axes[idx], leaf_font_size=11)\n",
    "    \n",
    "    axes[idx].set_title(f'{method.capitalize()} Linkage', fontsize=14, fontweight='bold')\n",
    "    axes[idx].set_xlabel('Student Name', fontsize=11)\n",
    "    axes[idx].set_ylabel('Distance', fontsize=11)\n",
    "    axes[idx].grid(True, alpha=0.3, axis='y')\n",
    "\n",
    "plt.suptitle('Comparing Linkage Methods ðŸ”—', fontsize=16, fontweight='bold', y=1.00)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ¤” Notice the differences:\")\n",
    "print(\"  â€¢ Single: May create long chains\")\n",
    "print(\"  â€¢ Complete: More compact clusters\")\n",
    "print(\"  â€¢ Average: Balanced approach\")\n",
    "print(\"  â€¢ Ward's: Most balanced, similar to K-Means\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 4: Dendrograms ðŸŽ„\n",
    "\n",
    "## What is a Dendrogram?\n",
    "\n",
    "A **dendrogram** is a tree diagram that shows:\n",
    "- How clusters were formed (bottom to top)\n",
    "- The distance at which clusters merged (height)\n",
    "- Relationships between all data points\n",
    "\n",
    "### How to Read a Dendrogram:\n",
    "\n",
    "```\n",
    "Height = Distance at which clusters merged\n",
    "\n",
    "    |                      â† High distance = very different\n",
    "    |                    \n",
    "----+----                  â† Cut here = 2 clusters\n",
    "    |    |              \n",
    "    |  ----+----           â† Cut here = 3 clusters\n",
    "    |  |   |   |        \n",
    "   [A] [B][C] [D]          â† Bottom = individual points\n",
    "```\n",
    "\n",
    "**The higher the connection, the more different the groups!**\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“Š Create a Beautiful Dendrogram"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linkage matrix using Ward's method\n",
    "Z = linkage(X, method='ward')\n",
    "\n",
    "# Create dendrogram\n",
    "plt.figure(figsize=(14, 8))\n",
    "dendrogram(\n",
    "    Z,\n",
    "    labels=student_names,\n",
    "    leaf_font_size=13,\n",
    "    color_threshold=15,  # Color different heights differently\n",
    ")\n",
    "\n",
    "plt.title('Student Study Habits - Hierarchical Clustering ðŸŒ³', fontsize=16, fontweight='bold', pad=20)\n",
    "plt.xlabel('Student Name', fontsize=13)\n",
    "plt.ylabel('Distance (How Different)', fontsize=13)\n",
    "\n",
    "# Add cut lines\n",
    "plt.axhline(y=20, color='red', linestyle='--', linewidth=2, label='Cut for 3 clusters')\n",
    "plt.axhline(y=10, color='orange', linestyle='--', linewidth=2, label='Cut for 4 clusters')\n",
    "\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ“– How to read this dendrogram:\")\n",
    "print(\"  1. Bottom = individual students\")\n",
    "print(\"  2. Height = how different groups are when they merge\")\n",
    "print(\"  3. Cut horizontally to get different numbers of clusters\")\n",
    "print(\"  4. Lower connections = more similar students\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸŽ¯ Interactive: Choose Number of Clusters\n",
    "\n",
    "Unlike K-Means, we can decide K **after** seeing the dendrogram!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def visualize_clusters_from_dendrogram(X, names, n_clusters, method='ward'):\n",
    "    \"\"\"\n",
    "    Cut dendrogram at height to get n_clusters and visualize\n",
    "    \"\"\"\n",
    "    # Perform clustering\n",
    "    Z = linkage(X, method=method)\n",
    "    labels = fcluster(Z, n_clusters, criterion='maxclust')\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "    \n",
    "    # Left: Dendrogram with cut line\n",
    "    dendrogram(Z, labels=names, ax=ax1, leaf_font_size=11, color_threshold=0)\n",
    "    \n",
    "    # Find the height to cut for n_clusters\n",
    "    if n_clusters > 1:\n",
    "        # Get merge distances\n",
    "        heights = Z[:, 2]\n",
    "        # Find cut height for n_clusters\n",
    "        cut_height = (heights[-(n_clusters-1)] + heights[-n_clusters]) / 2 if n_clusters <= len(heights) else heights[-1]\n",
    "        ax1.axhline(y=cut_height, color='red', linestyle='--', linewidth=2, \n",
    "                   label=f'Cut for {n_clusters} clusters')\n",
    "    \n",
    "    ax1.set_title(f'Dendrogram (Cut for {n_clusters} clusters)', fontsize=13, fontweight='bold')\n",
    "    ax1.set_xlabel('Student Name')\n",
    "    ax1.set_ylabel('Distance')\n",
    "    ax1.legend()\n",
    "    ax1.grid(True, alpha=0.3, axis='y')\n",
    "    \n",
    "    # Right: Scatter plot with cluster colors\n",
    "    scatter = ax2.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', \n",
    "                         s=300, edgecolors='black', linewidth=2, alpha=0.7)\n",
    "    \n",
    "    # Add labels\n",
    "    for i, name in enumerate(names):\n",
    "        ax2.annotate(name, (X[i, 0], X[i, 1]), fontsize=10, ha='center', va='bottom')\n",
    "    \n",
    "    ax2.set_title(f'Resulting {n_clusters} Clusters', fontsize=13, fontweight='bold')\n",
    "    ax2.set_xlabel('Study Hours per Week')\n",
    "    ax2.set_ylabel('Test Score')\n",
    "    ax2.grid(True, alpha=0.3)\n",
    "    plt.colorbar(scatter, ax=ax2, label='Cluster')\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print cluster assignments\n",
    "    print(f\"\\nðŸ“Š Cluster Assignments ({n_clusters} clusters):\")\n",
    "    for cluster_id in range(1, n_clusters + 1):\n",
    "        members = [names[i] for i, label in enumerate(labels) if label == cluster_id]\n",
    "        print(f\"  Cluster {cluster_id}: {members}\")\n",
    "\n",
    "# Try different numbers of clusters\n",
    "print(\"Let's try cutting the dendrogram at different heights:\\n\")\n",
    "\n",
    "for k in [2, 3, 4]:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(f\"K = {k} clusters\")\n",
    "    print('='*60)\n",
    "    visualize_clusters_from_dendrogram(X, student_names, k)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ðŸ’¡ Which K to Choose?\n",
    "\n",
    "Look at the dendrogram:\n",
    "- **Large vertical gaps** = good place to cut\n",
    "- **Small vertical gaps** = points are similar, don't separate\n",
    "\n",
    "For our student data:\n",
    "- **K=3** seems natural: Low/Medium/High study groups\n",
    "- **K=2** also works: Low vs. High performers\n",
    "- **K=4** might be over-splitting\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Part 5: Using Scikit-learn ðŸš€\n",
    "\n",
    "Now let's use the professional library!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import AgglomerativeClustering\n",
    "\n",
    "# Create model\n",
    "n_clusters = 3\n",
    "model = AgglomerativeClustering(\n",
    "    n_clusters=n_clusters,\n",
    "    linkage='ward'  # Try: 'single', 'complete', 'average', 'ward'\n",
    ")\n",
    "\n",
    "# Fit and predict\n",
    "labels = model.fit_predict(X)\n",
    "\n",
    "print(f\"ðŸŽ¯ Hierarchical Clustering with K={n_clusters}\")\n",
    "print(\"=\"*60)\n",
    "\n",
    "# Show cluster assignments\n",
    "for i, (name, label) in enumerate(zip(student_names, labels)):\n",
    "    print(f\"{name:10s} â†’ Cluster {label} | Study: {X[i, 0]:.1f}h, Score: {X[i, 1]:.0f}\")\n",
    "\n",
    "# Visualize\n",
    "plt.figure(figsize=(10, 6))\n",
    "scatter = plt.scatter(X[:, 0], X[:, 1], c=labels, cmap='viridis', \n",
    "                     s=300, edgecolors='black', linewidth=2, alpha=0.7)\n",
    "\n",
    "# Add labels\n",
    "for i, name in enumerate(student_names):\n",
    "    plt.annotate(name, (X[i, 0], X[i, 1]), fontsize=11, ha='center', va='bottom')\n",
    "\n",
    "plt.xlabel('Study Hours per Week', fontsize=12)\n",
    "plt.ylabel('Test Score', fontsize=12)\n",
    "plt.title(f'Hierarchical Clustering Results (K={n_clusters})', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Cluster')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nâœ… Scikit-learn makes it easy!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 6: K-Means vs Hierarchical Comparison ðŸ†š"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create non-spherical data to show differences\n",
    "from sklearn.datasets import make_moons\n",
    "\n",
    "X_moons, _ = make_moons(n_samples=200, noise=0.05, random_state=42)\n",
    "\n",
    "# Apply both algorithms\n",
    "kmeans = KMeans(n_clusters=2, random_state=42)\n",
    "hierarchical = AgglomerativeClustering(n_clusters=2, linkage='average')\n",
    "\n",
    "labels_kmeans = kmeans.fit_predict(X_moons)\n",
    "labels_hierarchical = hierarchical.fit_predict(X_moons)\n",
    "\n",
    "# Visualize comparison\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 6))\n",
    "\n",
    "# K-Means\n",
    "ax1.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_kmeans, cmap='viridis', \n",
    "           s=50, edgecolors='black', linewidths=0.5, alpha=0.7)\n",
    "ax1.scatter(kmeans.cluster_centers_[:, 0], kmeans.cluster_centers_[:, 1], \n",
    "           c='red', marker='X', s=300, edgecolors='black', linewidths=2, label='Centroids')\n",
    "ax1.set_title('K-Means Clustering', fontsize=13, fontweight='bold')\n",
    "ax1.set_xlabel('Feature 1')\n",
    "ax1.set_ylabel('Feature 2')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Hierarchical\n",
    "ax2.scatter(X_moons[:, 0], X_moons[:, 1], c=labels_hierarchical, cmap='viridis', \n",
    "           s=50, edgecolors='black', linewidths=0.5, alpha=0.7)\n",
    "ax2.set_title('Hierarchical Clustering (Average Linkage)', fontsize=13, fontweight='bold')\n",
    "ax2.set_xlabel('Feature 1')\n",
    "ax2.set_ylabel('Feature 2')\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('K-Means vs Hierarchical on Crescent-Shaped Data ðŸŒ™', \n",
    "            fontsize=15, fontweight='bold', y=1.02)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸŽ¯ Observations:\")\n",
    "print(\"  â€¢ K-Means tries to make spherical clusters (struggles here!)\")\n",
    "print(\"  â€¢ Hierarchical with average linkage handles the shape better\")\n",
    "print(\"  â€¢ Choice of linkage method matters!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 7: Real-World Example - Customer Segmentation ðŸ›ï¸"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create synthetic customer data\n",
    "np.random.seed(42)\n",
    "\n",
    "customer_data = {\n",
    "    'Customer': [f'Cust_{i:02d}' for i in range(1, 21)],\n",
    "    'Annual_Income': np.random.randint(20, 120, 20),  # in $1000s\n",
    "    'Spending_Score': np.random.randint(1, 100, 20)   # 1-100 scale\n",
    "}\n",
    "\n",
    "df_customers = pd.DataFrame(customer_data)\n",
    "\n",
    "print(\"ðŸ›ï¸ Customer Segmentation Data:\")\n",
    "print(df_customers.head(10))\n",
    "print(f\"\\nTotal customers: {len(df_customers)}\")\n",
    "\n",
    "# Prepare data\n",
    "X_customers = df_customers[['Annual_Income', 'Spending_Score']].values\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_customers_scaled = scaler.fit_transform(X_customers)\n",
    "\n",
    "print(\"\\nâœ… Features scaled for clustering\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Dendrogram to Choose K"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create linkage matrix\n",
    "Z_customers = linkage(X_customers_scaled, method='ward')\n",
    "\n",
    "# Plot dendrogram\n",
    "plt.figure(figsize=(16, 8))\n",
    "dendrogram(\n",
    "    Z_customers,\n",
    "    labels=df_customers['Customer'].values,\n",
    "    leaf_font_size=10,\n",
    "    color_threshold=7\n",
    ")\n",
    "\n",
    "plt.title('Customer Segmentation Dendrogram ðŸ›ï¸', fontsize=15, fontweight='bold', pad=20)\n",
    "plt.xlabel('Customer ID', fontsize=12)\n",
    "plt.ylabel('Distance', fontsize=12)\n",
    "\n",
    "# Add potential cut lines\n",
    "plt.axhline(y=8, color='red', linestyle='--', linewidth=2, label='Cut for 3 segments')\n",
    "plt.axhline(y=6, color='orange', linestyle='--', linewidth=2, label='Cut for 4 segments')\n",
    "\n",
    "plt.legend(fontsize=11)\n",
    "plt.grid(True, alpha=0.3, axis='y')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nðŸ’¡ Based on the dendrogram, 3-4 customer segments seem natural\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Apply Clustering and Analyze Segments"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Choose K=3 based on dendrogram\n",
    "n_segments = 3\n",
    "\n",
    "# Create model\n",
    "model = AgglomerativeClustering(n_clusters=n_segments, linkage='ward')\n",
    "segments = model.fit_predict(X_customers_scaled)\n",
    "\n",
    "# Add to dataframe\n",
    "df_customers['Segment'] = segments\n",
    "\n",
    "# Visualize segments\n",
    "plt.figure(figsize=(12, 7))\n",
    "scatter = plt.scatter(df_customers['Annual_Income'], \n",
    "                     df_customers['Spending_Score'],\n",
    "                     c=segments, cmap='viridis', s=200, \n",
    "                     edgecolors='black', linewidth=1.5, alpha=0.7)\n",
    "\n",
    "plt.xlabel('Annual Income ($1000s)', fontsize=12)\n",
    "plt.ylabel('Spending Score (1-100)', fontsize=12)\n",
    "plt.title(f'Customer Segmentation ({n_segments} Segments) ðŸŽ¯', fontsize=14, fontweight='bold')\n",
    "plt.colorbar(scatter, label='Segment')\n",
    "plt.grid(True, alpha=0.3)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Analyze each segment\n",
    "print(\"\\nðŸ“Š Customer Segment Analysis:\")\n",
    "print(\"=\"*70)\n",
    "\n",
    "for seg in range(n_segments):\n",
    "    seg_data = df_customers[df_customers['Segment'] == seg]\n",
    "    print(f\"\\nðŸŽ¯ Segment {seg}:\")\n",
    "    print(f\"  Size: {len(seg_data)} customers ({len(seg_data)/len(df_customers)*100:.1f}%)\")\n",
    "    print(f\"  Avg Income: ${seg_data['Annual_Income'].mean():.1f}k\")\n",
    "    print(f\"  Avg Spending Score: {seg_data['Spending_Score'].mean():.1f}\")\n",
    "    print(f\"  Customers: {', '.join(seg_data['Customer'].values[:5])}{'...' if len(seg_data) > 5 else ''}\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70)\n",
    "print(\"ðŸ’¡ Use these segments for targeted marketing!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Part 8: Practice Exercises ðŸŽ¯\n",
    "\n",
    "## Exercise 1: Different Linkage Methods\n",
    "\n",
    "Try all four linkage methods on the customer data. Which gives the most meaningful segments?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# Try: 'single', 'complete', 'average', 'ward'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 2: Determine Optimal K\n",
    "\n",
    "Create a dendrogram and determine the optimal number of clusters for a new dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create new dataset\n",
    "X_new, _ = make_blobs(n_samples=100, centers=4, n_features=2, \n",
    "                      cluster_std=1.0, random_state=42)\n",
    "\n",
    "# YOUR CODE HERE\n",
    "# 1. Create a dendrogram\n",
    "# 2. Decide optimal K\n",
    "# 3. Apply clustering\n",
    "# 4. Visualize results\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Exercise 3: Compare K-Means vs Hierarchical\n",
    "\n",
    "Apply both algorithms to the same data and compare results:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# YOUR CODE HERE\n",
    "# 1. Apply K-Means\n",
    "# 2. Apply Hierarchical (choose linkage)\n",
    "# 3. Create side-by-side visualizations\n",
    "# 4. Which performs better? Why?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# Summary & Key Takeaways ðŸ“š\n",
    "\n",
    "## What We Learned:\n",
    "\n",
    "âœ… **Hierarchical Clustering** creates a family tree of data  \n",
    "âœ… **Agglomerative** (bottom-up) is most common  \n",
    "âœ… **Linkage methods** determine how clusters merge  \n",
    "âœ… **Dendrograms** visualize the hierarchy  \n",
    "âœ… **No need to specify K** beforehand - decide from dendrogram  \n",
    "âœ… **Different from K-Means** - shows relationships, not just groups  \n",
    "\n",
    "## When to Use Hierarchical Clustering:\n",
    "\n",
    "âœ“ When you want to understand **data structure** and relationships  \n",
    "âœ“ When you **don't know K** beforehand  \n",
    "âœ“ When dataset is **small to medium** (<10,000 points)  \n",
    "âœ“ When you need a **hierarchy** for interpretation  \n",
    "âœ“ When clusters might have **different shapes** (with right linkage)  \n",
    "\n",
    "## Linkage Method Cheat Sheet:\n",
    "\n",
    "- **Single**: Find chain-like patterns, sensitive to noise\n",
    "- **Complete**: Compact clusters, sensitive to outliers\n",
    "- **Average**: Balanced, usually a safe choice\n",
    "- **Ward's**: Similar to K-Means, creates balanced sizes\n",
    "\n",
    "## Comparison:\n",
    "\n",
    "| Feature | K-Means | Hierarchical |\n",
    "|---------|---------|-------------|\n",
    "| Speed | âš¡ Very Fast | ðŸŒ Slower |\n",
    "| Specify K? | Yes | No (cut dendrogram) |\n",
    "| Hierarchy? | No | Yes (dendrogram) |\n",
    "| Interpretability | Good | Excellent |\n",
    "| Scalability | Excellent | Poor |\n",
    "\n",
    "---\n",
    "\n",
    "## ðŸŽ‰ Congratulations!\n",
    "\n",
    "You now understand hierarchical clustering and can:\n",
    "- Create and interpret dendrograms\n",
    "- Choose appropriate linkage methods\n",
    "- Determine optimal number of clusters\n",
    "- Apply to real-world problems\n",
    "- Compare with K-Means intelligently\n",
    "\n",
    "**Next:** Learn about density-based clustering (DBSCAN & OPTICS)! ðŸŽ¨\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ðŸ“š Additional Resources\n",
    "\n",
    "### Further Reading:\n",
    "- [Scikit-learn: Hierarchical Clustering](https://scikit-learn.org/stable/modules/clustering.html#hierarchical-clustering)\n",
    "- [SciPy: Dendrograms](https://docs.scipy.org/doc/scipy/reference/cluster.hierarchy.html)\n",
    "- Understanding different linkage methods in depth\n",
    "\n",
    "### Practice Datasets:\n",
    "- **Iris Dataset** - Classic clustering example\n",
    "- **Wine Dataset** - Multi-feature hierarchical clustering\n",
    "- **Customer Data** - Real-world segmentation\n",
    "\n",
    "### Next Steps:\n",
    "1. Complete all exercises in this notebook\n",
    "2. Try hierarchical clustering on your own datasets\n",
    "3. Experiment with different linkage methods\n",
    "4. Compare results with K-Means\n",
    "5. Move on to Density-Based Clustering notebook\n",
    "\n",
    "---\n",
    "\n",
    "**Happy Clustering! ðŸŒ³**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
