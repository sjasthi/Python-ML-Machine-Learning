# ü§ñ Support Vector Machine (SVM)
### Python for Machine Learning ‚Äî Middle School Edition

---

> **What we'll learn today:**  
> How computers can draw an invisible line to sort things into groups ‚Äî and why that's super powerful!

---

## üìã Table of Contents

1. [What is SVM? ‚Äî The Big Idea](#1-what-is-svm--the-big-idea)
2. [A Fun Analogy: Sorting Fruit](#2-a-fun-analogy-sorting-fruit)
3. [Key Vocabulary](#3-key-vocabulary)
4. [Hyperplanes ‚Äî Finding the Best Line](#4-hyperplanes--finding-the-best-line)
5. [Support Vectors ‚Äî The VIP Points](#5-support-vectors--the-vip-points)
6. [The Margin ‚Äî Why Bigger is Better](#6-the-margin--why-bigger-is-better)
7. [The Kernel Trick ‚Äî Seeing in 3D](#7-the-kernel-trick--seeing-in-3d)
8. [Types of Kernels](#8-types-of-kernels)
9. [Hard Margin vs Soft Margin (The C Parameter)](#9-hard-margin-vs-soft-margin-the-c-parameter)
10. [SVM In Action: Cats vs Dogs](#10-svm-in-action-cats-vs-dogs)
11. [Real-World Uses of SVM](#11-real-world-uses-of-svm)
12. [Multi-Class Classification with SVM](#12-multi-class-classification-with-svm)
    - 12a. [One-vs-Rest (OvR)](#12a-one-vs-rest-ovr)
    - 12b. [One-vs-One (OvO)](#12b-one-vs-one-ovo)
    - 12c. [OvR vs OvO ‚Äî Which to Use?](#12c-ovr-vs-ovo--which-to-use)
    - 12d. [Full Multi-Class Code Example](#12d-full-multi-class-code-example)
13. [Full Python Code Example (Binary)](#13-full-python-code-example-binary)
14. [How to Use SVM ‚Äî Step by Step](#14-how-to-use-svm--step-by-step)
15. [Practice Challenges](#15-practice-challenges)
16. [Quick Summary](#16-quick-summary)

---

## 1. What is SVM? ‚Äî The Big Idea

**Support Vector Machine (SVM)** is a machine learning algorithm that helps a computer **classify** (sort) things into groups.

Think of it like this: Imagine you have a big table covered with red and blue marbles all mixed together. Your job is to draw a line on the table so that all the red marbles are on one side and all the blue marbles are on the other side. **SVM finds the best possible line to do exactly that!**

But SVM doesn't just find *any* line ‚Äî it finds the **smartest line**: the one that keeps as much space as possible between the marbles and the line. This makes it much better at sorting marbles it has never seen before.

> **In real life, SVM can classify:**
> - Spam emails vs. real emails
> - Cat photos vs. dog photos
> - Healthy cells vs. cancer cells
> - Happy reviews vs. angry reviews

---

## 2. A Fun Analogy: Sorting Fruit

Imagine a school cafeteria where apples and oranges keep getting mixed together on the conveyor belt. The lunch staff draws a line on the belt ‚Äî everything to the left goes in the apple bin, everything to the right goes in the orange bin.

SVM is the computer version of that cafeteria worker ‚Äî except it finds the **perfect** dividing line automatically!

![Fruit Sorting Analogy](images_svm/01_fruit_analogy.png)

**What you see in the picture above:**
- üî¥ Red circles = Apples
- üü† Orange diamonds = Oranges
- The solid black line = The **decision boundary** (the dividing line)
- The shaded blue zone = The **margin** (the safety space on either side of the line)

Notice how the line doesn't just *barely* squeeze between them ‚Äî it stays as far away as possible from both the apples and the oranges. That's what makes SVM special!

---

## 3. Key Vocabulary

Before we go further, let's learn the important words we'll use today:

| Term | What it Means | Everyday Analogy |
|------|---------------|------------------|
| **Classification** | Sorting things into groups | Sorting your Lego blocks by color |
| **Decision Boundary** | The line that separates the two groups | The fence between two yards |
| **Hyperplane** | The "line" that separates data (can be in many dimensions) | A fence in 2D, a wall in 3D |
| **Margin** | The gap between the boundary and the closest data points | The safety lane on a highway |
| **Support Vectors** | The data points closest to the decision boundary | The players right next to the half-court line |
| **Kernel** | A math trick to handle tricky curved data | Putting on 3D glasses to see more clearly |

---

## 4. Hyperplanes ‚Äî Finding the Best Line

When we have data in 2D (like a regular graph with x and y), the decision boundary is a **line**.

But when we have 3D data (x, y, and z), the boundary becomes a **flat plane** (like a sheet of paper floating in space).

In general, for any number of dimensions, this boundary is called a **hyperplane**.

Here's the tricky part: **Many different lines could separate the data.** So which one should SVM pick?

![Hyperplane Concept](images_svm/02_hyperplane_concept.png)

**Left side:** Many possible lines can separate the red dots from the blue triangles. But which is best?

**Right side:** SVM picks the line that creates the **widest margin** ‚Äî the biggest gap between the two groups. This line is the most confident separator!

### Why does the widest margin matter?

Think of it like a tight-rope walker. If the rope is right in the middle of two buildings with lots of space on either side, a small gust of wind won't make them fall. But if the rope is right at the edge, even a tiny wobble is dangerous.

The wide margin = stability = better predictions on new data!

---

## 5. Support Vectors ‚Äî The VIP Points

Not all data points are equally important to SVM. The ones that matter most are the ones **closest to the decision boundary**. These special points are called **Support Vectors**.

![Support Vectors](images_svm/03_support_vectors.png)

**Gold-bordered points = Support Vectors** ‚Äî these are the "VIP" data points that actually determine where the decision boundary goes!

### Key facts about Support Vectors:
- They are the points that are hardest to classify (they're right on the edge)
- If you remove any other point, the boundary stays the same
- If you move a support vector, the boundary moves too!
- SVM literally *rests* on these points ‚Äî that's why they're called "support" vectors

> **Sports Analogy:** In basketball, the players standing closest to the half-court line define where "your side" vs "their side" begins. The players way in the back don't affect that boundary. Support vectors are like those players closest to the line!

---

## 6. The Margin ‚Äî Why Bigger is Better

The **margin** is the total distance between the two margin boundary lines (the dashed lines in the diagrams). SVM's entire goal is to **maximize** this margin.

```
    Class A         |              |         Class B
    ‚óè ‚óè    ‚óè        |              |       ‚ñ≤     ‚ñ≤ ‚ñ≤
       ‚óè  ‚óè  [‚óè]   |‚Üê ‚Üê margin ‚Üí |‚Üí [‚ñ≤]  ‚ñ≤    ‚ñ≤
    ‚óè                |              |
                  Support         Support
                  Vector          Vector
                  (closest A)     (closest B)
```

The margin = distance from the decision boundary to the nearest support vector on **each** side.

### Bigger margin = Better generalization

If the margin is wide, new data points have more room to land in the correct zone even if they're slightly different from the training data. A tight margin leaves no room for error!

---

## 7. The Kernel Trick ‚Äî Seeing in 3D

Sometimes data can't be separated by any straight line. Imagine mixing red and blue sprinkles on a plate ‚Äî the red ones are in the middle and the blue ones surround them in a circle. No straight line separates them!

This is where the **Kernel Trick** comes in ‚Äî it's the most magical part of SVM!

![Kernel Trick](images_svm/04_kernel_trick.png)

**The kernel trick "lifts" the data into a higher dimension** where it suddenly *can* be separated by a flat surface.

### Simple Analogy:

Imagine you have red and blue coins mixed together on a table. From above (2D view), you can't draw a line to separate them. But if you **slam the table** (add energy/a new dimension), the red coins fly up higher than the blue coins. Now from the side, you can easily see a horizontal line separating them!

The kernel function is like that table slam ‚Äî it transforms the data so separation becomes possible.

**Common types of kernel transformations:**
- Takes 2D data ‚Üí computes new 3D coordinates
- A point at (x, y) might become (x¬≤, y¬≤, ‚àö2¬∑x¬∑y)
- In that new space, a flat plane separates what curved lines couldn't!

---

## 8. Types of Kernels

SVM comes with different "lenses" you can put on to see the data differently:

![Types of Kernels](images_svm/06_kernels.png)

### Linear Kernel
- **What it does:** Draws a straight line
- **Best for:** Data that can already be separated by a line
- **Code:** `SVC(kernel='linear')`
- **Like:** Sorting people into "left side" and "right side" of a room

### Polynomial Kernel
- **What it does:** Draws a curved boundary (like a U-shape or S-curve)
- **Best for:** Data with curved relationships
- **Code:** `SVC(kernel='poly', degree=3)`
- **Like:** Drawing a curved fence around a playground

### RBF Kernel (Radial Basis Function)
- **What it does:** Can draw circular or blob-shaped boundaries
- **Best for:** Most real-world problems ‚Äî it's the default!
- **Code:** `SVC(kernel='rbf')`
- **Like:** Drawing a spotlight circle around a group of people

> **Tip for beginners:** When in doubt, start with `kernel='rbf'` ‚Äî it works well for most problems!

---

## 9. Hard Margin vs Soft Margin (The C Parameter)

What if your data has a few weird points that are in the wrong zone? Should SVM be super strict and try to separate *everything* perfectly, or should it be a bit flexible?

This is controlled by the **C parameter**:

![Soft vs Hard Margin](images_svm/08_soft_vs_hard_margin.png)

| Setting | What It Means | Risk |
|---------|---------------|------|
| **Small C** (like `C=0.1`) | "Be flexible ‚Äî allow some mistakes" | Might be too simple |
| **Large C** (like `C=100`) | "Be strict ‚Äî no mistakes allowed" | Might memorize the data (overfitting) |

### The Goldilocks Problem

- Too strict (large C) ‚Üí SVM memorizes every single training point ‚Üí fails on new data
- Too flexible (small C) ‚Üí SVM ignores important patterns ‚Üí also fails on new data
- Just right ‚Üí SVM learns the general pattern ‚Üí works great on new data!

> **Analogy:** Imagine studying for a test. If you memorize only the exact practice problems (large C), you'll fail when the test has different questions. If you study too loosely (small C), you won't know anything. The best approach is to learn the *concepts* ‚Äî that's what a good C value does!

### How to find the right C:

```python
# Try different C values and see which gives the best accuracy
from sklearn.model_selection import GridSearchCV

param_grid = {'C': [0.01, 0.1, 1, 10, 100]}
grid = GridSearchCV(SVC(kernel='rbf'), param_grid, cv=5)
grid.fit(X_train, y_train)
print("Best C:", grid.best_params_)
```

---

## 10. SVM In Action: Cats vs Dogs

Let's see SVM solving a real classification problem: figuring out if an animal is a cat or a dog based on its **weight** and **height**!

![Cats vs Dogs SVM](images_svm/05_cats_vs_dogs.png)

**Left plot:** The SVM model learns from training data and draws a decision boundary between cats (purple circles) and dogs (orange triangles).

**Right plot:** When we give it new animals (the star points), it correctly predicts whether they're cats or dogs!

### What the shading means:
- üü¢ Green zone ‚Üí SVM predicts "Dog"
- üî¥ Red zone ‚Üí SVM predicts "Cat"
- The boundary between them is where SVM is "50-50" unsure

---

## 11. Real-World Uses of SVM

SVM is used in many amazing applications all around you:

![Real World Uses](images_svm/07_real_world_uses.png)

### Email Spam Detection
Every time your email app moves junk mail to the spam folder, something like SVM might be running behind the scenes! It looks at words in the email to decide: *Spam or Not Spam?*

### Face Recognition
Unlocking your phone with your face? SVM can learn what your face features look like and separate "your face" from "not your face."

### Medical Diagnosis
Doctors use SVM to help detect diseases. For example, looking at a cell sample and predicting: *Healthy or Cancerous?* SVM has helped save lives!

### Plant Disease Detection
Farmers use image-based SVM tools to look at a leaf photo and determine: *Healthy plant or Diseased plant?* This helps catch problems early!

### Sentiment Analysis
When a company reads thousands of online reviews, SVM can sort them: *Happy customer or Unhappy customer?*

### Self-Driving Cars
Sensors detect objects, and SVM helps classify them: *Person? Car? Stop sign? Animal?* making real-time safety decisions.

---

## 12. Multi-Class Classification with SVM

So far, everything we've learned has been about sorting things into **two** groups (yes/no, cat/dog, spam/not-spam). But real life often has more than two categories! What if you want to classify:

- Three types of flowers (Setosa, Versicolor, Virginica)?
- Four genres of music (Rock, Pop, Jazz, Classical)?
- Five grades on a test (A, B, C, D, F)?

SVM was originally designed for **binary** (two-class) problems. To handle **multiple classes**, it uses clever strategies that break the big problem into many smaller binary problems and then combines the answers.

There are two main strategies: **One-vs-Rest (OvR)** and **One-vs-One (OvO)**.

---

### 12a. One-vs-Rest (OvR)

**The Big Idea:** Train one binary classifier for each class. Each classifier asks the question: *"Is this data point THIS class, or one of the rest?"*

**Analogy:** Imagine you're at a pizza party trying to find out who ordered what. You go around and ask: "Did you order pepperoni? Did you order cheese? Did you order veggie?" You ask one question per topping. The answer that has the highest confidence wins!

![OvR Concept](images_svm/10_ovr_concept.png)

In the image above, three separate classifiers are trained on the Iris flower dataset. Each one highlights **one flower type in color** and treats **all others as grey "rest"** points.

#### How OvR works step by step:

**Example: 3 flower types ‚Üí 3 classifiers are trained**

```
Classifier 1: Setosa (YES) vs [Versicolor + Virginica] (NO)
Classifier 2: Versicolor (YES) vs [Setosa + Virginica] (NO)
Classifier 3: Virginica (YES) vs [Setosa + Versicolor] (NO)
```

When a new flower arrives:
1. Run it through all 3 classifiers
2. Each classifier gives a **confidence score** (how sure is it?)
3. The class with the **highest confidence score** wins!

```
New flower ‚Üí 
  Classifier 1 (Setosa):    confidence = 0.12  ‚Üê low
  Classifier 2 (Versicolor): confidence = 0.71  ‚Üê highest!
  Classifier 3 (Virginica):  confidence = 0.17  ‚Üê low

Final answer: Versicolor  (highest confidence)
```

#### Number of classifiers needed:

| Classes | Classifiers Needed |
|---------|-------------------|
| 3       | 3                 |
| 4       | 4                 |
| 10      | 10                |
| N       | **N**             |

#### Python code for OvR:

```python
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier

# Wrap SVC inside OneVsRestClassifier
model_ovr = OneVsRestClassifier(SVC(kernel='rbf', C=1.0))
model_ovr.fit(X_train, y_train)
y_pred = model_ovr.predict(X_test)
```

> **Note:** When you use `OneVsRestClassifier`, sklearn automatically trains one SVM per class behind the scenes. You don't need to do anything extra!

---

### 12b. One-vs-One (OvO)

**The Big Idea:** Instead of each class fighting "the world", hold a **tournament** where every pair of classes faces each other in a head-to-head match. The class that wins the most matches is the final answer!

**Analogy:** Think of a round-robin sports tournament. Every team plays every other team. At the end, whoever won the most games is the champion. OvO works the same way!

![OvO Concept](images_svm/11_ovo_concept.png)

In the image above, each subplot shows a different head-to-head match: Setosa vs Versicolor, Setosa vs Virginica, and Versicolor vs Virginica. Each match only uses the data from those two classes.

#### How OvO works step by step:

**Example: 3 flower types ‚Üí 3 matches are held**

```
Match 1: Setosa vs Versicolor    ‚Üí Winner: Setosa
Match 2: Setosa vs Virginica     ‚Üí Winner: Setosa
Match 3: Versicolor vs Virginica ‚Üí Winner: Versicolor

Vote tally:
  Setosa:    2 wins  ‚Üê WINNER!
  Versicolor: 1 win
  Virginica:  0 wins
```

![OvO Voting Process](images_svm/13_ovo_voting.png)

The image above shows a 4-class music genre example. After 6 matches, each genre's wins are counted and the class with the most wins becomes the final prediction.

#### Number of classifiers needed:

| Classes | Classifiers Needed | Formula           |
|---------|-------------------|-------------------|
| 3       | 3                 | 3 √ó 2 / 2 = 3     |
| 4       | 6                 | 4 √ó 3 / 2 = 6     |
| 5       | 10                | 5 √ó 4 / 2 = 10    |
| N       | **N √ó (N-1) / 2** | General formula   |

#### Python code for OvO:

```python
from sklearn.svm import SVC

# SVC uses OvO by default ‚Äî no extra wrapper needed!
model_ovo = SVC(kernel='rbf', C=1.0, decision_function_shape='ovo')
model_ovo.fit(X_train, y_train)
y_pred = model_ovo.predict(X_test)
```

> **Fun fact:** When you use `SVC()` in sklearn, it already uses One-vs-One under the hood! You don't need to do anything special for OvO with SVC.

---

### 12c. OvR vs OvO ‚Äî Which to Use?

![OvR vs OvO Comparison](images_svm/12_ovr_vs_ovo_comparison.png)

Here's a simple guide to help you choose:

| Situation | Best Choice | Why |
|-----------|-------------|-----|
| Many classes (10+) | **OvR** | Fewer classifiers to train |
| Few classes (2‚Äì5) | **OvO** | More accurate, smaller training sets per classifier |
| Using SVC in sklearn | **OvO** | It's the default ‚Äî just use `SVC()` |
| Need probability scores | **OvR** | Works better with `predict_proba` |
| Imbalanced classes | **OvR** | Better handles unequal class sizes |

**The short answer for beginners:** Just use `SVC()` ‚Äî it uses OvO automatically and works great for most problems!

---

### 12d. Full Multi-Class Code Example

Let's use the famous **Iris flower dataset** ‚Äî 150 flowers, 3 species, 4 features. This is one of the most popular datasets for learning machine learning!

```python
# ============================================================
# Multi-Class SVM: Classifying 3 Types of Iris Flowers
# ============================================================

import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.model_selection import train_test_split
from sklearn.preprocessing import StandardScaler
from sklearn.metrics import accuracy_score, classification_report, confusion_matrix, ConfusionMatrixDisplay

# Step 1: Load the Iris dataset
iris = load_iris()
X = iris.data       # 4 features: sepal length, sepal width, petal length, petal width
y = iris.target     # 3 classes: 0=Setosa, 1=Versicolor, 2=Virginica

print("Dataset Info:")
print(f"  Total flowers: {len(X)}")
print(f"  Features per flower: {X.shape[1]} ‚Üí {list(iris.feature_names)}")
print(f"  Classes: {list(iris.target_names)}")
print(f"  Flowers per class: {[sum(y==i) for i in range(3)]}")

# Step 2: Split into train and test
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)

# Step 3: Scale the features
scaler = StandardScaler()
X_train_sc = scaler.fit_transform(X_train)
X_test_sc  = scaler.transform(X_test)

# Step 4a: Train using OvO (SVC default)
model_ovo = SVC(kernel='rbf', C=2.0, decision_function_shape='ovo', random_state=42)
model_ovo.fit(X_train_sc, y_train)
y_pred_ovo = model_ovo.predict(X_test_sc)

# Step 4b: Train using OvR
model_ovr = OneVsRestClassifier(SVC(kernel='rbf', C=2.0, random_state=42))
model_ovr.fit(X_train_sc, y_train)
y_pred_ovr = model_ovr.predict(X_test_sc)

# Step 5: Compare results
print("\n--- One-vs-One (OvO) Results ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred_ovo)*100:.1f}%")
print(classification_report(y_test, y_pred_ovo, target_names=iris.target_names))

print("--- One-vs-Rest (OvR) Results ---")
print(f"Accuracy: {accuracy_score(y_test, y_pred_ovr)*100:.1f}%")
print(classification_report(y_test, y_pred_ovr, target_names=iris.target_names))

# Step 6: Visualize the confusion matrix (OvO)
fig, axes = plt.subplots(1, 2, figsize=(12, 5))

for ax, y_pred, title in zip(axes,
    [y_pred_ovo, y_pred_ovr],
    ['OvO Confusion Matrix', 'OvR Confusion Matrix']):
    cm = confusion_matrix(y_test, y_pred)
    disp = ConfusionMatrixDisplay(cm, display_labels=iris.target_names)
    disp.plot(ax=ax, colorbar=False, cmap='Blues')
    ax.set_title(title, fontsize=12, fontweight='bold')

plt.suptitle('Multi-Class SVM: How Many Did Each Model Get Right?', fontsize=13, fontweight='bold')
plt.tight_layout()
plt.show()

# Step 7: Predict a brand-new flower!
new_flower = np.array([[5.1, 3.5, 1.4, 0.2]])   # typical Setosa measurements
new_scaled  = scaler.transform(new_flower)
pred = model_ovo.predict(new_scaled)
print(f"\nNew flower measurements: {new_flower[0]}")
print(f"Prediction: {iris.target_names[pred[0]]}!")
```

#### Expected Output:
```
Dataset Info:
  Total flowers: 150
  Features per flower: 4 ‚Üí ['sepal length (cm)', ...]
  Classes: ['setosa', 'versicolor', 'virginica']
  Flowers per class: [50, 50, 50]

--- One-vs-One (OvO) Results ---
Accuracy: 96.7%
              precision    recall  f1-score
    setosa       1.00      1.00      1.00
versicolor       0.94      0.94      0.94
 virginica       0.94      0.94      0.94

--- One-vs-Rest (OvR) Results ---
Accuracy: 96.7%

New flower measurements: [5.1 3.5 1.4 0.2]
Prediction: setosa!
```

#### Visualizing the Decision Boundaries (using only 2 features):

```python
# Use only petal length & petal width for easy 2D visualization
X_2d = iris.data[:, [2, 3]]   # columns 2 and 3
X_2d_sc = StandardScaler().fit_transform(X_2d)

# Plot OvO boundary
clf = SVC(kernel='rbf', C=2.0)
clf.fit(X_2d_sc, iris.target)

xx, yy = np.meshgrid(np.linspace(-2.5, 2.5, 300), np.linspace(-2.5, 2.5, 300))
Z = clf.predict(np.c_[xx.ravel(), yy.ravel()]).reshape(xx.shape)

plt.figure(figsize=(8, 6))
plt.contourf(xx, yy, Z, alpha=0.25, cmap=plt.cm.Pastel1, levels=[-0.5, 0.5, 1.5, 2.5])
plt.contour(xx, yy, Z, colors='grey', linewidths=1.5, levels=[0.5, 1.5])

colors = ['#e74c3c', '#3498db', '#27ae60']
for i, (color, name) in enumerate(zip(colors, iris.target_names)):
    plt.scatter(X_2d_sc[iris.target==i, 0], X_2d_sc[iris.target==i, 1],
                s=80, color=color, label=name, edgecolors='white')

plt.xlabel('Petal Length (scaled)')
plt.ylabel('Petal Width (scaled)')
plt.title('SVM Multi-Class Decision Boundaries (Iris)', fontsize=13, fontweight='bold')
plt.legend()
plt.show()
```

![Multi-Class Decision Boundaries](images_svm/14_multiclass_result.png)

Both OvR (left) and OvO (right) produce nearly identical boundaries on this dataset. The colored regions show which zone SVM assigns to each flower type.

#### Reading the Confusion Matrix:

![Confusion Matrix](images_svm/15_confusion_matrix.png)

The confusion matrix is a grid that shows how well the model did:
- Each **row** represents the actual (true) flower type
- Each **column** represents what the model predicted
- Numbers on the **diagonal** (top-left to bottom-right) = **correct predictions** ‚úÖ
- Numbers **off the diagonal** = mistakes ‚ùå

In the example above, SVM correctly identified nearly all flowers. The small number of errors happens between Versicolor and Virginica ‚Äî even for humans, those two look very similar!

---

## 13. Full Python Code Example (Binary)

Here's a complete example you can run in your Jupyter notebook! We'll classify whether a student is "likely to pass" or "needs more practice" based on their homework score and quiz score.

```python
# ============================================================
# SVM Example: Will a Student Pass or Need More Practice?
# ============================================================

# Step 1: Import the libraries we need
import numpy as np
import matplotlib.pyplot as plt
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score, classification_report
from sklearn.preprocessing import StandardScaler

# Step 2: Create our sample data
# Features: [homework_score (0-100), quiz_score (0-100)]
# Label: 0 = Needs Practice, 1 = Likely to Pass

np.random.seed(42)

# Students who passed (high scores)
pass_hw    = np.random.normal(78, 8, 50)   # homework scores
pass_quiz  = np.random.normal(75, 8, 50)   # quiz scores

# Students needing practice (lower scores)
needs_hw   = np.random.normal(48, 8, 50)
needs_quiz = np.random.normal(45, 8, 50)

# Combine into one dataset
X = np.vstack([
    np.column_stack([pass_hw, pass_quiz]),
    np.column_stack([needs_hw, needs_quiz])
])
y = np.array([1]*50 + [0]*50)   # 1 = pass, 0 = needs practice

# Step 3: Split into training and testing sets
X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
print(f"Training samples: {len(X_train)}")
print(f"Testing samples:  {len(X_test)}")

# Step 4: Scale the features (very important for SVM!)
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled  = scaler.transform(X_test)

# Step 5: Create and train the SVM model
model = SVC(kernel='rbf', C=1.0, random_state=42)
model.fit(X_train_scaled, y_train)
print("\nModel trained successfully!")

# Step 6: Make predictions
y_pred = model.predict(X_test_scaled)

# Step 7: Evaluate accuracy
accuracy = accuracy_score(y_test, y_pred)
print(f"\nModel Accuracy: {accuracy * 100:.1f}%")
print("\nDetailed Report:")
print(classification_report(y_test, y_pred,
      target_names=['Needs Practice', 'Likely to Pass']))

# Step 8: Test with a new student!
new_student = np.array([[65, 70]])   # homework=65, quiz=70
new_scaled  = scaler.transform(new_student)
prediction  = model.predict(new_scaled)

label = "Likely to Pass! üéâ" if prediction[0] == 1 else "Needs More Practice üìö"
print(f"\nNew student (HW=65, Quiz=70): {label}")

# Step 9: Visualize the decision boundary
plt.figure(figsize=(10, 7))

# Create a mesh to plot the decision boundary
h = 0.5
x_min, x_max = X[:, 0].min() - 5, X[:, 0].max() + 5
y_min, y_max = X[:, 1].min() - 5, X[:, 1].max() + 5
xx, yy = np.meshgrid(np.arange(x_min, x_max, h),
                     np.arange(y_min, y_max, h))

mesh_scaled = scaler.transform(np.c_[xx.ravel(), yy.ravel()])
Z = model.predict(mesh_scaled).reshape(xx.shape)

plt.contourf(xx, yy, Z, alpha=0.3, cmap='RdYlGn')
plt.contour(xx, yy, Z, colors='black', linewidths=2)

# Plot the data points
plt.scatter(X[y==0, 0], X[y==0, 1], color='red',   label='Needs Practice', s=80, alpha=0.7)
plt.scatter(X[y==1, 0], X[y==1, 1], color='green', label='Likely to Pass',  s=80, alpha=0.7)
plt.scatter(65, 70, color='blue', s=300, marker='*', zorder=5, label='New Student')

plt.xlabel('Homework Score', fontsize=12)
plt.ylabel('Quiz Score', fontsize=12)
plt.title('SVM: Will the Student Pass? üéì', fontsize=14, fontweight='bold')
plt.legend(fontsize=11)
plt.tight_layout()
plt.show()
```

### Expected Output:
```
Training samples: 80
Testing samples:  20
Model trained successfully!

Model Accuracy: 97.5%

New student (HW=65, Quiz=70): Likely to Pass! üéâ
```

---

## 14. How to Use SVM ‚Äî Step by Step

![SVM Steps Flowchart](images_svm/09_svm_steps_flowchart.png)

Here's the recipe to build any SVM model:

**Step 1 ‚Äî Collect Data:** Gather examples with labels (e.g., "spam" or "not spam" emails)

**Step 2 ‚Äî Prepare Features:** Choose which measurements/attributes to use (word count, sender, subject line...)

**Step 3 ‚Äî Split: Train & Test:** Keep 80% of data to teach the model, 20% to test it

**Step 4 ‚Äî Scale the Features:** SVM works best when all features are on the same scale (0 to 1 or -1 to 1)

**Step 5 ‚Äî Create the Model:** `model = SVC(kernel='rbf', C=1.0)`

**Step 6 ‚Äî Train the Model:** `model.fit(X_train_scaled, y_train)`

**Step 7 ‚Äî Make Predictions:** `y_pred = model.predict(X_test_scaled)`

**Step 8 ‚Äî Check Accuracy:** `accuracy_score(y_test, y_pred)`

### The Minimal SVM Template:

```python
from sklearn.svm import SVC
from sklearn.preprocessing import StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# 1. Split
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)

# 2. Scale (very important!)
scaler = StandardScaler()
X_train = scaler.fit_transform(X_train)
X_test  = scaler.transform(X_test)

# 3. Create, Train, Predict
model = SVC(kernel='rbf', C=1.0)
model.fit(X_train, y_train)
y_pred = model.predict(X_test)

# 4. Score
print(f"Accuracy: {accuracy_score(y_test, y_pred) * 100:.1f}%")
```

> **Important reminder:** Always scale your features before using SVM! SVM is very sensitive to features with very different sizes. For example, if one feature goes from 0‚Äì100 and another from 0‚Äì1,000,000, the big one will dominate. `StandardScaler` fixes this!

---

## 15. Practice Challenges

### Challenge 1 ‚Äî Beginner: Classify Shapes üîµüî∫
```python
# Data: circles vs triangles based on radius and angle
import numpy as np
from sklearn.svm import SVC
from sklearn.metrics import accuracy_score

np.random.seed(0)
circles   = np.column_stack([np.random.uniform(1, 5, 30),   np.random.uniform(0, 360, 30)])
triangles = np.column_stack([np.random.uniform(6, 10, 30),  np.random.uniform(0, 360, 30)])

X = np.vstack([circles, triangles])
y = [0]*30 + [1]*30

# TODO: Build and train an SVM model to classify circles vs triangles!
# Hint: Use SVC(kernel='rbf')
```

### Challenge 2 ‚Äî Intermediate: Tune the C Parameter üéõÔ∏è
Train the student score example from earlier with different C values (`0.01, 0.1, 1, 10, 100`) and record the accuracy for each. Which C value works best? Make a bar chart!

### Challenge 3 ‚Äî Advanced: Multi-class SVM üå∏
SVM can classify more than 2 groups! Use the built-in Iris flower dataset to classify 3 types of flowers:

```python
from sklearn.datasets import load_iris
from sklearn.svm import SVC
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score

# Load data
iris = load_iris()
X, y = iris.data, iris.target

# TODO:
# 1. Split into train/test
# 2. Scale the features
# 3. Train SVC with kernel='rbf'
# 4. Print accuracy
# 5. Try to visualize using just 2 of the 4 features
```

### Challenge 4 ‚Äî Explorer: Real Dataset üìä
Download the SMS Spam dataset from the internet and use `CountVectorizer` to turn text into numbers, then train SVM to detect spam!

---

## 16. Quick Summary

| Concept | Key Point |
|---------|-----------|
| **SVM Goal** | Find the decision boundary with the **widest margin** |
| **Hyperplane** | The dividing line/plane/surface between classes |
| **Support Vectors** | The most important points ‚Äî closest to the boundary |
| **Margin** | The gap between the boundary and support vectors ‚Äî maximize it! |
| **Kernel** | Math trick to handle non-linearly separable data |
| **C Parameter** | Controls strictness: small C = flexible, large C = strict |
| **Scaling** | Always scale features before SVM! |
| **Default Kernel** | Use `rbf` when you're not sure |
| **Multi-class** | SVM extends to 3+ classes using OvR or OvO strategies |
| **OvR** | N classifiers ‚Äî each class vs all the rest; pick highest confidence |
| **OvO** | N√ó(N-1)/2 classifiers ‚Äî every pair fights; most match wins wins |
| **sklearn default** | `SVC()` uses OvO automatically |

### SVM Cheat Sheet:
```python
from sklearn.svm import SVC
from sklearn.multiclass import OneVsRestClassifier
from sklearn.preprocessing import StandardScaler

# Scale first!
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# Binary classification (2 classes)
model = SVC(kernel='rbf', C=1.0)

# Multi-class ‚Äî OvO (SVC default, recommended)
model = SVC(kernel='rbf', C=1.0, decision_function_shape='ovo')

# Multi-class ‚Äî OvR (explicit)
model = OneVsRestClassifier(SVC(kernel='rbf', C=1.0))

# Train and predict (same for all!)
model.fit(X_train, y_train)
predictions = model.predict(X_new)
```

---

## üß† Reflect and Review

Before you leave today, think about these questions:

1. **In your own words**, what does SVM do? Explain it to someone who has never heard of it.
2. Why do we want the **widest** margin and not just any margin?
3. What are **support vectors** and why are they called "support" vectors?
4. When would you use an **RBF kernel** instead of a linear kernel?
5. If your SVM model has 99% accuracy on training data but only 60% on test data, which C value might you try ‚Äî larger or smaller? Why?
6. SVM was originally designed for **two** classes. How does **One-vs-Rest** let it handle three or more classes?
7. In **One-vs-One**, how many classifiers would you need to tell apart **5 different types of music** genres? Show your calculation using the formula N √ó (N-1) / 2.

---

## üìö Further Reading

- [Scikit-learn SVM Documentation](https://scikit-learn.org/stable/modules/svm.html)
- [SVM in 2 minutes - Video](https://www.youtube.com/watch?v=_YPScrckx28)
- [SVM - Kernel Trick](https://www.youtube.com/watch?v=Q7vT0--5VII)
- - Try searching: *"SVM interactive demo"* to find websites where you can drag points and watch the boundary update live!

---

*Happy Coding! Remember: Every expert was once a beginner. Keep experimenting!* üöÄ

---

**Course:** Python for Machine Learning | **Level:** Middle School  
**Topic:** Support Vector Machines (SVM) | **Images folder:** `images_svm/`
