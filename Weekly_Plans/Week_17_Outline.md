# Week 17: Ensemble Methods - Boosting, Stacking, and Voting

**Python for Machine Learning Course**  
**Metro State University | Learn and Help Program**  
**Instructor: Siva**

---

## ðŸ“… Week Overview

This week builds on Week 16's Random Forests lesson by introducing three more powerful ensemble techniques: Boosting, Stacking, and Voting.

---

## ðŸŽ¯ Learning Objectives

By the end of this week, students will be able to:

1. Explain how Boosting differs from Bagging (sequential vs parallel)
2. Describe the two-level architecture of Stacking
3. Compare Hard Voting and Soft Voting
4. Implement ensemble methods using scikit-learn
5. Choose appropriate ensemble methods for different problems

---

## ðŸ“š Key Concepts

- **Boosting**: Sequential learning where each model fixes previous mistakes
- **Stacking**: Two-level learning with base models and a meta-learner
- **Voting**: Democratic combination of model predictions (hard and soft)
- **Model Diversity**: Why different types of models strengthen ensembles

---

## ðŸ“– Resources

### Main Presentation
[ML_Ensemble_Methods_Boosting_Stacking_and_Voting.md](https://github.com/sjasthi/Python-ML-Machine-Learning/blob/main/Presentations/ML_Ensemble_Methods_Boosting_Stacking_and_Voting.md)
- Theory and explanations
- Visual diagrams
- Real-world analogies
- Code examples

### Hands-On Notebook
[ML_Ensemble_Methods_Boosting_Stacking_and_Voting.ipynb](https://github.com/sjasthi/Python-ML-Machine-Learning/blob/main/Colab_Notebooks/ML_Ensemble_Methods_Boosting_Stacking_and_Voting.ipynb)
- Complete implementation examples
- Breast Cancer dataset analysis
- Performance comparisons
- Practice exercises

### Interactive Learning & Quiz
[ML_Ensemble_Methods_Interactive_Playbook_and_Quiz.html](https://github.com/sjasthi/Python-ML-Machine-Learning/blob/main/Play/ML_Ensemble_Methods_Interactive_Playbook_and_Quiz.html)
- Interactive demonstrations
- 10-question quiz with explanations
- Screenshot score for submission

---

## ðŸ“… Weekly Schedule

### Day 1: Introduction & Boosting
- **Topics**: Recap Random Forests, Introduction to Boosting
- **Activities**:
  - Review presentation: Boosting section
  - Discuss sequential learning concept
  - Interactive demo: Boosting tab
- **Homework**: Read through Boosting examples in notebook

### Day 2: Stacking
- **Topics**: Two-level ensemble architecture
- **Activities**:
  - Review presentation: Stacking section
  - Explore meta-learning concept
  - Interactive demo: Stacking tab
- **Homework**: Experiment with Stacking code in notebook

### Day 3: Voting & Comparison
- **Topics**: Hard vs Soft Voting, Method comparison
- **Activities**:
  - Review presentation: Voting section
  - Compare all four ensemble methods
  - Interactive demos: Voting tabs
- **Homework**: Complete notebook exercises

### Day 4: Practice & Assessment
- **Topics**: Implementation practice, Quiz
- **Activities**:
  - Work through complete notebook comparison
  - Take the interactive quiz
  - Screenshot and submit score to Google Classroom
- **Homework**: Finish any incomplete exercises

---

## âœ… Assignments

###  Interactive Quiz (10 points)
Complete the interactive quiz:
- [ ] Take the 10-question quiz
- [ ] Screenshot your final score
- [ ] Submit screenshot to Google Classroom

**Due**: End of Week 17

---

## ðŸ”— Additional Resources

- **Scikit-learn Ensemble Documentation**: https://scikit-learn.org/stable/modules/ensemble.html
- **XGBoost Documentation**: https://xgboost.readthedocs.io/
- **Kaggle Ensemble Method Tutorials**: Search "ensemble methods" on Kaggle

---

## ðŸ¤” Discussion Questions

1. Why does Boosting train models sequentially instead of in parallel?
2. How does Stacking's meta-model know how to combine base model predictions?
3. When would Soft Voting be better than Hard Voting?
4. Why is model diversity so important in ensemble methods?

---

*Keep learning and experimenting! ðŸš€*
